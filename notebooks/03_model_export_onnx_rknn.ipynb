{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5L7lAwn8UcCD"
      },
      "source": [
        "# PART 3: ONNX EXPORT & RKNN INT8 QUANTIZATION\n",
        "\n",
        "## Hooks Match Part 2\n",
        "\n",
        "This notebook:\n",
        "- Loads `best_classifier.pt` with correct hook architecture\n",
        "- Provides proper save/load functions\n",
        "- Includes inference examples\n",
        "- Exports to ONNX\n",
        "- Prepares for RKNN quantization\n",
        "\n",
        "---"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Jk1A5A0KUcCL",
        "outputId": "8f14d3e3-3d16-47ac-e5a4-2d5047db351c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "                    INSTALLING REQUIRED PACKAGES\n",
            "================================================================================\n",
            "\n",
            "·Ä§ Installing ONNX, ONNXRuntime, and ONNXScript...\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m689.1/689.1 kB\u001b[0m \u001b[31m17.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ‚îÅ\u001b[0m \u001b[32m159.3/159.3 kB\u001b[0m \u001b[31m14.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h‚úÖ All packages installed!\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "# CELL 0: Install Required Packages\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\" \" * 20 + \"INSTALLING REQUIRED PACKAGES\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "print(\"\\n·Ä§ Installing ONNX, ONNXRuntime, and ONNXScript...\")\n",
        "!pip install -q onnx onnxruntime onnxscript\n",
        "\n",
        "print(\"‚úÖ All packages installed!\\n\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QjZiqBweUcCO",
        "outputId": "caa04e59-ef77-4028-f026-e5b635256f16"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "================================================================================\n",
            "                    PART 3: MODEL EXPORT & QUANTIZATION\n",
            "================================================================================\n",
            "\n",
            "üì± Device: cpu\n",
            "   PyTorch version: 2.9.0+cpu\n",
            "   ONNX version: 1.20.1\n",
            "   ONNXRuntime version: 1.24.1\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "# CELL 1: Setup and Imports\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\" \" * 20 + \"PART 3: MODEL EXPORT & QUANTIZATION\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import numpy as np\n",
        "import cv2\n",
        "from pathlib import Path\n",
        "import onnx\n",
        "import onnxruntime as ort\n",
        "from tqdm import tqdm\n",
        "import matplotlib.pyplot as plt\n",
        "from PIL import Image\n",
        "import shutil\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Check CUDA availability\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(f\"\\nüì± Device: {device}\")\n",
        "print(f\"   PyTorch version: {torch.__version__}\")\n",
        "print(f\"   ONNX version: {onnx.__version__}\")\n",
        "print(f\"   ONNXRuntime version: {ort.__version__}\")\n",
        "\n",
        "# Configure matplotlib for Colab\n",
        "%matplotlib inline\n",
        "plt.rcParams['figure.dpi'] = 100"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d3DJjQd-UcCP",
        "outputId": "256291a7-be7e-422e-fcff-b258e82d7ade"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "üèóÔ∏è  CREATING MODEL ARCHITECTURE FILE (EXACT COPY FROM PART 2)\n",
            "================================================================================\n",
            "‚úÖ Created __init__.py in models directory\n",
            "‚úÖ Architecture file created!\n",
            "   Location: models/yolo_with_classifier.py\n",
            "   ‚úÖ MATCHES PART 2 TRAINING EXACTLY\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "# CELL 2: Create Model Architecture File (MATCHES PART 2 EXACTLY)\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üèóÔ∏è  CREATING MODEL ARCHITECTURE FILE (EXACT COPY FROM PART 2)\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Ensure we're in /content\n",
        "os.chdir('/content')\n",
        "\n",
        "# Create models directory\n",
        "models_dir = Path('models')\n",
        "models_dir.mkdir(exist_ok=True)\n",
        "\n",
        "# IMPORTANT: Create __init__.py to make it a Python package\n",
        "(models_dir / '__init__.py').touch()\n",
        "print(f\"‚úÖ Created __init__.py in models directory\")\n",
        "\n",
        "# Architecture code - EXACT COPY FROM PART 2\n",
        "architecture_code = '''\n",
        "# ==============================================================================\n",
        "# models/yolo_with_classifier.py\n",
        "# ==============================================================================\n",
        "# Multi-Head YOLO: Frozen Detection Model + Trainable Classification Head\n",
        "# THIS IS THE EXACT ARCHITECTURE FROM PART 2 TRAINING NOTEBOOK\n",
        "# ==============================================================================\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "\n",
        "\n",
        "class ClassificationHead(nn.Module):\n",
        "    \"\"\"Multi-scale classification head.\"\"\"\n",
        "\n",
        "    def __init__(self, nc_cls=3, input_channels=[64, 128, 256]):\n",
        "        super().__init__()\n",
        "        self.nc_cls = nc_cls\n",
        "\n",
        "        # Create a classifier for each scale\n",
        "        self.classifiers = nn.ModuleList()\n",
        "\n",
        "        for channels in input_channels:\n",
        "            classifier = nn.Sequential(\n",
        "                nn.AdaptiveAvgPool2d(1),\n",
        "                nn.Flatten(),\n",
        "                nn.Linear(channels, 128),\n",
        "                nn.ReLU(),\n",
        "                nn.Dropout(0.2),\n",
        "                nn.Linear(128, nc_cls)\n",
        "            )\n",
        "            self.classifiers.append(classifier)\n",
        "\n",
        "    def forward(self, features):\n",
        "        \"\"\"Forward pass using multi-scale features.\"\"\"\n",
        "        outputs = []\n",
        "        for i, feat in enumerate(features):\n",
        "            out = self.classifiers[i](feat)\n",
        "            outputs.append(out)\n",
        "\n",
        "        # Average outputs from different scales\n",
        "        final_output = torch.stack(outputs, dim=0).mean(dim=0)\n",
        "        return final_output\n",
        "\n",
        "\n",
        "class ModelWithClassifier(nn.Module):\n",
        "    \"\"\"YOLOv5 model with added classification head.\"\"\"\n",
        "\n",
        "    def __init__(self, detection_model, nc_cls=3, freeze_detection=True):\n",
        "        \"\"\"\n",
        "        Args:\n",
        "            detection_model: Pretrained YOLOv5 detection model\n",
        "            nc_cls: Number of classification classes\n",
        "            freeze_detection: Whether to freeze detection layers\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "\n",
        "        self.detection_model = detection_model\n",
        "        self.nc_cls = nc_cls\n",
        "        self.features = []  # Store intermediate features\n",
        "\n",
        "        # Freeze detection model (backbone + neck + detection head)\n",
        "        if freeze_detection:\n",
        "            for param in self.detection_model.parameters():\n",
        "                param.requires_grad = False\n",
        "            print(\"‚úÖ Froze all detection model parameters\")\n",
        "\n",
        "        # Find the indices of P3, P4, P5 layers (outputs from neck going to detection head)\n",
        "        # In YOLOv5, the Detect layer takes inputs from specific layers\n",
        "        detect_layer = self.detection_model.model[-1]\n",
        "        self.feature_indices = detect_layer.f  # Indices of layers that feed into Detect\n",
        "\n",
        "        # Register forward hooks to capture features from these layers\n",
        "        self.hooks = []\n",
        "        for idx in self.feature_indices:\n",
        "            layer = self.detection_model.model[idx]\n",
        "            hook = layer.register_forward_hook(self._hook_fn)\n",
        "            self.hooks.append(hook)\n",
        "\n",
        "        # Get input channels for classification head\n",
        "        input_channels = []\n",
        "        for idx in self.feature_indices:\n",
        "            # Get output channels from each layer\n",
        "            layer = self.detection_model.model[idx]\n",
        "            if hasattr(layer, 'cv3'):  # C3 layer\n",
        "                ch = layer.cv3.conv.out_channels\n",
        "            elif hasattr(layer, 'conv'):  # Conv layer\n",
        "                ch = layer.conv.out_channels\n",
        "            else:\n",
        "                ch = 256  # Default fallback\n",
        "            input_channels.append(ch)\n",
        "\n",
        "        # Add classification head\n",
        "        self.classifier = ClassificationHead(nc_cls=nc_cls, input_channels=input_channels)\n",
        "\n",
        "        print(f\"‚úÖ Added classification head (nc_cls={nc_cls})\")\n",
        "        print(f\"   Input channels: {input_channels}\")\n",
        "\n",
        "    def _hook_fn(self, module, input, output):\n",
        "        \"\"\"Hook function to capture intermediate features.\"\"\"\n",
        "        self.features.append(output)\n",
        "\n",
        "    def forward(self, x, get_features=False):\n",
        "        \"\"\"\n",
        "        Forward pass through detection model and classification head.\n",
        "\n",
        "        Args:\n",
        "            x: Input images [batch, 3, H, W]\n",
        "            get_features: If True, return features for classification\n",
        "        \"\"\"\n",
        "        # Clear previous features\n",
        "        self.features = []\n",
        "\n",
        "        # Forward through detection model (hooks will capture features)\n",
        "        det_output = self.detection_model(x)\n",
        "\n",
        "        # Get classification predictions using captured features\n",
        "        if get_features or self.training:\n",
        "            cls_logits = self.classifier(self.features)\n",
        "            return det_output, cls_logits\n",
        "        else:\n",
        "            return det_output\n",
        "\n",
        "    def __del__(self):\n",
        "        \"\"\"Remove hooks when object is deleted.\"\"\"\n",
        "        for hook in self.hooks:\n",
        "            hook.remove()\n",
        "'''\n",
        "\n",
        "# Write architecture file\n",
        "with open(models_dir / 'yolo_with_classifier.py', 'w') as f:\n",
        "    f.write(architecture_code)\n",
        "\n",
        "print(f\"‚úÖ Architecture file created!\")\n",
        "print(f\"   Location: {models_dir / 'yolo_with_classifier.py'}\")\n",
        "print(f\"   ‚úÖ MATCHES PART 2 TRAINING EXACTLY\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AEsIEjApUcCQ",
        "outputId": "20d9d741-3e41-4983-dd03-8a48a6d61a25"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "üì¶ LOADING YOLOV5 AND MODEL ARCHITECTURE\n",
            "================================================================================\n",
            "‚úÖ Working directory: /content\n",
            "‚úÖ YOLOv5 already exists\n",
            "\n",
            "üìö Python path updated (Priority: /content)\n",
            "üîÑ Importing custom architecture...\n",
            "\n",
            "‚úÖ All modules loaded successfully!\n",
            "   ModelWithClassifier: <class 'yolo_with_classifier.ModelWithClassifier'>\n",
            "   ClassificationHead: <class 'yolo_with_classifier.ClassificationHead'>\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "# CELL 3: Load YOLOv5 and Import Architecture\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üì¶ LOADING YOLOV5 AND MODEL ARCHITECTURE\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "import os\n",
        "import sys\n",
        "from pathlib import Path\n",
        "import importlib.util\n",
        "\n",
        "# Ensure we're in /content\n",
        "os.chdir('/content')\n",
        "print(f\"‚úÖ Working directory: {os.getcwd()}\")\n",
        "\n",
        "# Clone YOLOv5 if needed\n",
        "if not os.path.exists('yolov5'):\n",
        "    print(\"\\n‚ùå YOLOv5 not found! Cloning repository...\")\n",
        "    !git clone https://github.com/ultralytics/yolov5\n",
        "    os.chdir('yolov5')\n",
        "    !pip install -qr requirements.txt\n",
        "    os.chdir('/content')\n",
        "    print(\"‚úÖ YOLOv5 installed!\")\n",
        "else:\n",
        "    print(\"‚úÖ YOLOv5 already exists\")\n",
        "\n",
        "# Add paths - we put /content FIRST to prioritize our custom models folder\n",
        "if '/content' not in sys.path:\n",
        "    sys.path.insert(0, '/content')\n",
        "\n",
        "yolov5_path = Path('/content/yolov5')\n",
        "if str(yolov5_path) not in sys.path:\n",
        "    sys.path.append(str(yolov5_path))\n",
        "\n",
        "print(f\"\\nüìö Python path updated (Priority: /content)\")\n",
        "\n",
        "# Import YOLOv5 components\n",
        "# We use the yolov5 namespace if necessary, or rely on path appending\n",
        "from models.yolo import DetectionModel\n",
        "from models.common import *\n",
        "\n",
        "# Robust Import for custom architecture to avoid namespace collisions\n",
        "print(\"üîÑ Importing custom architecture...\")\n",
        "custom_model_path = Path('/content/models/yolo_with_classifier.py')\n",
        "\n",
        "spec = importlib.util.spec_from_file_location(\"yolo_with_classifier\", str(custom_model_path))\n",
        "custom_module = importlib.util.module_from_spec(spec)\n",
        "sys.modules[\"yolo_with_classifier\"] = custom_module\n",
        "spec.loader.exec_module(custom_module)\n",
        "\n",
        "ModelWithClassifier = custom_module.ModelWithClassifier\n",
        "ClassificationHead = custom_module.ClassificationHead\n",
        "\n",
        "print(\"\\n‚úÖ All modules loaded successfully!\")\n",
        "print(f\"   ModelWithClassifier: {ModelWithClassifier}\")\n",
        "print(f\"   ClassificationHead: {ClassificationHead}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mVccAQeHUcCR"
      },
      "outputs": [],
      "source": [
        "# ==============================================================================\n",
        "# CELL 4: Upload Model Files\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üì§ UPLOAD MODEL FILES\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "os.chdir('/content')\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "required_files = {\n",
        "    'best_classifier.pt': 'Trained classifier model',\n",
        "    'baseline_best.pt': 'Baseline detection model'\n",
        "}\n",
        "\n",
        "for filename, description in required_files.items():\n",
        "    file_path = Path(filename)\n",
        "    if file_path.exists():\n",
        "        print(f\"‚úÖ Found: {filename} ({description})\")\n",
        "        print(f\"   Size: {file_path.stat().st_size / 1024 / 1024:.2f} MB\")\n",
        "    else:\n",
        "        print(f\"‚ùå Missing: {filename}\")\n",
        "        print(f\"   Please upload {filename}:\")\n",
        "        uploaded = files.upload()\n",
        "        if filename in uploaded:\n",
        "            print(f\"   ‚úÖ Uploaded {filename}\")\n",
        "\n",
        "print(\"\\n‚úÖ All required files ready!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Prf6PRhcZ7-j",
        "outputId": "87b02e8a-074b-4870-e911-65067c40fc51"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Module registered for checkpoint loading!\n",
            "   Available as: models.yolo_with_classifier\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "# QUICK FIX: Register custom module for checkpoint loading\n",
        "# ==============================================================================\n",
        "\n",
        "import sys\n",
        "import importlib.util\n",
        "from pathlib import Path\n",
        "\n",
        "custom_model_path = Path('/content/models/yolo_with_classifier.py')\n",
        "\n",
        "spec = importlib.util.spec_from_file_location(\"models.yolo_with_classifier\", str(custom_model_path))\n",
        "custom_module = importlib.util.module_from_spec(spec)\n",
        "\n",
        "# Register with exact name PyTorch expects\n",
        "sys.modules[\"models.yolo_with_classifier\"] = custom_module\n",
        "sys.modules[\"yolo_with_classifier\"] = custom_module\n",
        "spec.loader.exec_module(custom_module)\n",
        "\n",
        "# Add to models package\n",
        "if \"models\" in sys.modules:\n",
        "    sys.modules[\"models\"].yolo_with_classifier = custom_module\n",
        "\n",
        "print(\"‚úÖ Module registered for checkpoint loading!\")\n",
        "print(f\"   Available as: models.yolo_with_classifier\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6lIZ7v-a95N",
        "outputId": "1af6966f-1d96-4b42-fe2c-0f8443320b1e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Model config updated: nc=1\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "# QUICK FIX: Set correct number of classes\n",
        "# ==============================================================================\n",
        "\n",
        "import yaml\n",
        "from pathlib import Path\n",
        "\n",
        "yolov5_path = Path('/content/yolov5')\n",
        "\n",
        "# Read and modify the config\n",
        "with open(yolov5_path / 'models/yolov5n.yaml', 'r') as f:\n",
        "    model_cfg = yaml.safe_load(f)\n",
        "    model_cfg['nc'] = 1  # Single class: ball\n",
        "\n",
        "print(f\"‚úÖ Model config updated: nc={model_cfg['nc']}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "T-GiOLn7UcCT",
        "outputId": "c90b1a87-3a5b-483b-e478-ffd102022689"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n",
            "                 from  n    params  module                                  arguments                     \n",
            "  0                -1  1      1760  models.common.Conv                      [3, 16, 6, 2, 2]              \n",
            "  1                -1  1      4672  models.common.Conv                      [16, 32, 3, 2]                \n",
            "  2                -1  1      4800  models.common.C3                        [32, 32, 1]                   \n",
            "  3                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
            "  4                -1  2     29184  models.common.C3                        [64, 64, 2]                   \n",
            "  5                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
            "  6                -1  3    156928  models.common.C3                        [128, 128, 3]                 \n",
            "  7                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
            "  8                -1  1    296448  models.common.C3                        [256, 256, 1]                 \n",
            "  9                -1  1    164608  models.common.SPPF                      [256, 256, 5]                 \n",
            " 10                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
            " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
            " 13                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
            " 14                -1  1      8320  models.common.Conv                      [128, 64, 1, 1]               \n",
            " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
            " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
            " 17                -1  1     22912  models.common.C3                        [128, 64, 1, False]           \n",
            " 18                -1  1     36992  models.common.Conv                      [64, 64, 3, 2]                \n",
            " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
            " 20                -1  1     74496  models.common.C3                        [128, 128, 1, False]          \n",
            " 21                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
            " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
            " 23                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
            " 24      [17, 20, 23]  1      8118  models.yolo.Detect                      [1, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [64, 128, 256]]\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "üîß LOADING TRAINED MODEL\n",
            "================================================================================\n",
            "\n",
            "üì• Loading checkpoint...\n",
            "   Model: best_classifier.pt\n",
            "   Baseline: baseline_best.pt\n",
            "\n",
            "üìä Checkpoint Information:\n",
            "   Model type: Unknown\n",
            "   Validation accuracy: 61.904761904761905%\n",
            "   Training epoch: 2\n",
            "   Keys in checkpoint: ['epoch', 'model', 'val_acc', 'optimizer']\n",
            "\n",
            "üì• Loading baseline detection model...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Model summary: 214 layers, 1765270 parameters, 1765270 gradients, 4.2 GFLOPs\n",
            "\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Baseline model loaded (nc=1)\n",
            "\n",
            "üèóÔ∏è  Creating multi-head model with fresh hooks...\n",
            "‚úÖ Froze all detection model parameters\n",
            "‚úÖ Added classification head (nc_cls=3)\n",
            "   Input channels: [64, 128, 256]\n",
            "\n",
            "üì• Loading trained weights...\n",
            "   Format: Old (full model object)\n",
            "‚úÖ Weights loaded successfully\n",
            "\n",
            "‚úÖ MODEL LOADED AND READY!\n",
            "\n",
            "üìä Model Statistics:\n",
            "   Total parameters: 1,824,159\n",
            "   Trainable parameters: 58,889 (3.23%)\n",
            "   Hooks registered: 3\n",
            "   Feature indices: [17, 20, 23]\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "# CELL 5: Load Trained Model (ROBUST LOADING)\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üîß LOADING TRAINED MODEL\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "os.chdir('/content')\n",
        "\n",
        "model_path = Path('best_classifier.pt')\n",
        "baseline_path = Path('baseline_best.pt')\n",
        "\n",
        "print(f\"\\nüì• Loading checkpoint...\")\n",
        "print(f\"   Model: {model_path}\")\n",
        "print(f\"   Baseline: {baseline_path}\")\n",
        "\n",
        "# Load checkpoint (state_dict only, not full model)\n",
        "ckpt = torch.load(model_path, map_location=device, weights_only=False)\n",
        "\n",
        "print(f\"\\nüìä Checkpoint Information:\")\n",
        "print(f\"   Model type: {ckpt.get('model_type', 'Unknown')}\")\n",
        "print(f\"   Validation accuracy: {ckpt.get('val_acc', 'N/A')}%\")\n",
        "print(f\"   Training epoch: {ckpt.get('epoch', 'N/A')}\")\n",
        "print(f\"   Keys in checkpoint: {list(ckpt.keys())}\")\n",
        "\n",
        "# Load baseline detection model\n",
        "print(f\"\\nüì• Loading baseline detection model...\")\n",
        "baseline_ckpt = torch.load(baseline_path, map_location=device, weights_only=False)\n",
        "\n",
        "# IMPORTANT: Create model with nc=1 to match the baseline checkpoint\n",
        "# The baseline was trained with nc=1 (single class: \"ball\")\n",
        "import yaml\n",
        "with open(yolov5_path / 'models/yolov5n.yaml', 'r') as f:\n",
        "    model_cfg = yaml.safe_load(f)\n",
        "    model_cfg['nc'] = 1  # Set to 1 class (ball)\n",
        "\n",
        "baseline_model = DetectionModel(model_cfg)\n",
        "baseline_model.load_state_dict(baseline_ckpt['model'].state_dict())\n",
        "print(\"‚úÖ Baseline model loaded (nc=1)\")\n",
        "\n",
        "# Create multi-head model (this will create NEW clean hooks)\n",
        "print(f\"\\nüèóÔ∏è  Creating multi-head model with fresh hooks...\")\n",
        "model = ModelWithClassifier(\n",
        "    detection_model=baseline_model,\n",
        "    nc_cls=3,\n",
        "    freeze_detection=True\n",
        ")\n",
        "\n",
        "# Load ONLY the trained weights (not hooks)\n",
        "print(f\"\\nüì• Loading trained weights...\")\n",
        "\n",
        "# Handle both old and new checkpoint formats\n",
        "if 'model_state_dict' in ckpt:\n",
        "    # New format (from save_model_properly)\n",
        "    state_dict = ckpt['model_state_dict']\n",
        "    print(\"   Format: New (model_state_dict)\")\n",
        "elif 'model' in ckpt:\n",
        "    # Old format (from Part 2 training)\n",
        "    if hasattr(ckpt['model'], 'state_dict'):\n",
        "        state_dict = ckpt['model'].state_dict()\n",
        "        print(\"   Format: Old (full model object)\")\n",
        "    else:\n",
        "        # It's already a state_dict\n",
        "        state_dict = ckpt['model']\n",
        "        print(\"   Format: Direct state_dict\")\n",
        "else:\n",
        "    raise KeyError(\"Checkpoint doesn't contain 'model' or 'model_state_dict'\")\n",
        "\n",
        "model.load_state_dict(state_dict, strict=True)\n",
        "print(\"‚úÖ Weights loaded successfully\")\n",
        "\n",
        "model.to(device)\n",
        "model.eval()\n",
        "\n",
        "print(\"\\n‚úÖ MODEL LOADED AND READY!\")\n",
        "\n",
        "# Verify\n",
        "total_params = sum(p.numel() for p in model.parameters())\n",
        "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
        "print(f\"\\nüìä Model Statistics:\")\n",
        "print(f\"   Total parameters: {total_params:,}\")\n",
        "print(f\"   Trainable parameters: {trainable_params:,} ({100*trainable_params/total_params:.2f}%)\")\n",
        "print(f\"   Hooks registered: {len(model.hooks)}\")\n",
        "print(f\"   Feature indices: {model.feature_indices}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MB68D_SGUcCV",
        "outputId": "8e379fba-fb95-4e9c-bb7f-6f22c4def8aa"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "üß™ TESTING PYTORCH MODEL\n",
            "================================================================================\n",
            "\n",
            "üìù Test 1: Dummy Input\n",
            "   Input shape: torch.Size([1, 3, 640, 640])\n",
            "   Detection output shape: torch.Size([1, 25200, 6])\n",
            "   Classification logits shape: torch.Size([1, 3])\n",
            "   Classification logits: [[   -0.01657    -0.60598     0.82417]]\n",
            "   Predicted class: 2 (Tennis Ball)\n",
            "\n",
            "‚úÖ Model works correctly!\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "# CELL 6: Test PyTorch Model + Inference Example\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üß™ TESTING PYTORCH MODEL\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Define class names\n",
        "class_names = {0: 'Basketball', 1: 'Football', 2: 'Tennis Ball'}\n",
        "\n",
        "# Test 1: Dummy input\n",
        "print(\"\\nüìù Test 1: Dummy Input\")\n",
        "dummy_input = torch.randn(1, 3, 640, 640).to(device)\n",
        "print(f\"   Input shape: {dummy_input.shape}\")\n",
        "\n",
        "with torch.no_grad():\n",
        "    det_output, cls_logits = model(dummy_input, get_features=True)\n",
        "\n",
        "print(f\"   Detection output shape: {det_output[0].shape}\")\n",
        "print(f\"   Classification logits shape: {cls_logits.shape}\")\n",
        "print(f\"   Classification logits: {cls_logits.cpu().numpy()}\")\n",
        "pred_class = cls_logits.argmax(dim=1).item()\n",
        "print(f\"   Predicted class: {pred_class} ({class_names[pred_class]})\")\n",
        "\n",
        "print(\"\\n‚úÖ Model works correctly!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q4klgR5IUcCW",
        "outputId": "ec86412b-e166-4d82-b7ee-41b195dfd055"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "üíæ MODEL SAVE/LOAD UTILITY FUNCTIONS\n",
            "================================================================================\n",
            "\n",
            "üìù Example: Saving current model...\n",
            "‚úÖ Model saved to: best_classifier_clean.pt\n",
            "   Saved: state_dict only (no hooks)\n",
            "\n",
            "‚úÖ Model saved without hooks!\n",
            "\n",
            "üí° To load in a new notebook:\n",
            "   model, ckpt = load_model_properly('best_classifier_clean.pt', baseline_model, device)\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "# CELL 7: Proper Model Save/Load Functions\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üíæ MODEL SAVE/LOAD UTILITY FUNCTIONS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "def save_model_properly(model, save_path, additional_info=None):\n",
        "    \"\"\"\n",
        "    Save model WITHOUT hooks (only weights).\n",
        "    This avoids pickle issues when reloading.\n",
        "    \"\"\"\n",
        "    checkpoint = {\n",
        "        'model_state_dict': model.state_dict(),\n",
        "        'model_type': 'ModelWithClassifier',\n",
        "        'nc_cls': model.nc_cls,\n",
        "        'feature_indices': model.feature_indices\n",
        "    }\n",
        "\n",
        "    if additional_info:\n",
        "        checkpoint.update(additional_info)\n",
        "\n",
        "    torch.save(checkpoint, save_path)\n",
        "    print(f\"‚úÖ Model saved to: {save_path}\")\n",
        "    print(f\"   Saved: state_dict only (no hooks)\")\n",
        "    return save_path\n",
        "\n",
        "\n",
        "def load_model_properly(checkpoint_path, baseline_model, device='cpu'):\n",
        "    \"\"\"\n",
        "    Load model by creating fresh architecture + loading weights.\n",
        "    This ensures clean hooks are created.\n",
        "    \"\"\"\n",
        "    # Load checkpoint\n",
        "    ckpt = torch.load(checkpoint_path, map_location=device, weights_only=False)\n",
        "\n",
        "    # Create model with fresh hooks\n",
        "    model = ModelWithClassifier(\n",
        "        detection_model=baseline_model,\n",
        "        nc_cls=ckpt.get('nc_cls', 3),\n",
        "        freeze_detection=True\n",
        "    )\n",
        "\n",
        "    # Load weights\n",
        "    model.load_state_dict(ckpt['model_state_dict'])\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    print(f\"‚úÖ Model loaded from: {checkpoint_path}\")\n",
        "    return model, ckpt\n",
        "\n",
        "\n",
        "# Example: Save current model properly\n",
        "print(\"\\nüìù Example: Saving current model...\")\n",
        "saved_path = save_model_properly(\n",
        "    model,\n",
        "    'best_classifier_clean.pt',\n",
        "    additional_info={\n",
        "        'val_acc': ckpt.get('val_acc', 0),\n",
        "        'epoch': ckpt.get('epoch', 0)\n",
        "    }\n",
        ")\n",
        "\n",
        "print(f\"\\n‚úÖ Model saved without hooks!\")\n",
        "print(f\"\\nüí° To load in a new notebook:\")\n",
        "print(f\"   model, ckpt = load_model_properly('best_classifier_clean.pt', baseline_model, device)\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rNpxeYLFUcCX",
        "outputId": "20623ecf-0446-4068-8aa2-dbcc7b515bc4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "üñºÔ∏è  COMPLETE INFERENCE EXAMPLE\n",
            "================================================================================\n",
            "\n",
            "üì¶ Extracting dataset from zip file...\n",
            "‚úÖ Dataset extracted!\n",
            "\n",
            "‚úÖ Found images at: /content/merged_ball_dataset/images\n",
            "   Total images: 210\n",
            "\n",
            "üì∏ Testing on 3 random images:\n",
            "\n",
            "‚ùå football_0115.jpg\n",
            "   Ground Truth: Football\n",
            "   Predicted: Tennis Ball (35.2%)\n",
            "   Logits: [  0.0048325   -0.006846    0.082921]\n",
            "\n",
            "‚úÖ football_0106.jpg\n",
            "   Ground Truth: Football\n",
            "   Predicted: Football (61.1%)\n",
            "   Logits: [   0.055982     0.74035      -1.292]\n",
            "\n",
            "‚úÖ tennis_0037.jpg\n",
            "   Ground Truth: Tennis Ball\n",
            "   Predicted: Tennis Ball (68.5%)\n",
            "   Logits: [   0.051534     -1.0136      1.1223]\n",
            "\n",
            "‚úÖ Inference example complete!\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "# CELL 8: Complete Inference Example on Real Image\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üñºÔ∏è  COMPLETE INFERENCE EXAMPLE\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "import torchvision.transforms as T\n",
        "import zipfile\n",
        "\n",
        "# Extract dataset if zip file exists\n",
        "zip_path = Path('/content/ball_multiclass_dataset.zip')\n",
        "if zip_path.exists() and not Path('/content/ball_multiclass_dataset').exists():\n",
        "    print(\"\\nüì¶ Extracting dataset from zip file...\")\n",
        "    with zipfile.ZipFile(zip_path, 'r') as zip_ref:\n",
        "        zip_ref.extractall('/content')\n",
        "    print(\"‚úÖ Dataset extracted!\")\n",
        "\n",
        "def run_inference(image_path, model, device):\n",
        "    \"\"\"\n",
        "    Complete inference example.\n",
        "\n",
        "    Args:\n",
        "        image_path: Path to image file\n",
        "        model: Loaded ModelWithClassifier\n",
        "        device: torch device\n",
        "\n",
        "    Returns:\n",
        "        detection_output, classification_result\n",
        "    \"\"\"\n",
        "    # Load image\n",
        "    img = cv2.imread(str(image_path))\n",
        "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Preprocess\n",
        "    transform = T.Compose([\n",
        "        T.ToPILImage(),\n",
        "        T.Resize((640, 640)),\n",
        "        T.ToTensor()\n",
        "    ])\n",
        "    img_tensor = transform(img_rgb).unsqueeze(0).to(device)\n",
        "\n",
        "    # Inference\n",
        "    with torch.no_grad():\n",
        "        det_output, cls_logits = model(img_tensor, get_features=True)\n",
        "\n",
        "    # Get classification result\n",
        "    pred_class = cls_logits.argmax(dim=1).item()\n",
        "    confidence = torch.softmax(cls_logits, dim=1).max().item()\n",
        "\n",
        "    result = {\n",
        "        'predicted_class': pred_class,\n",
        "        'class_name': class_names[pred_class],\n",
        "        'confidence': confidence,\n",
        "        'logits': cls_logits.cpu().numpy()\n",
        "    }\n",
        "\n",
        "    return det_output, result\n",
        "\n",
        "# Find images - NO train/val split, just images/ folder\n",
        "dataset_paths = [\n",
        "    Path('/content/ball_multiclass_dataset/images'),\n",
        "    Path('/content/ball_multiclass_dataset/merged_ball_dataset/images'),\n",
        "    Path('/content/merged_ball_dataset/images'),\n",
        "]\n",
        "\n",
        "img_dir = None\n",
        "for path in dataset_paths:\n",
        "    if path.exists():\n",
        "        img_dir = path\n",
        "        print(f\"\\n‚úÖ Found images at: {path}\")\n",
        "        break\n",
        "\n",
        "if img_dir and img_dir.exists():\n",
        "    # Get random sample of images\n",
        "    all_images = list(img_dir.glob('*.jpg')) + list(img_dir.glob('*.png'))\n",
        "\n",
        "    if len(all_images) > 0:\n",
        "        import random\n",
        "        random.seed(42)\n",
        "        test_images = random.sample(all_images, min(3, len(all_images)))\n",
        "\n",
        "        print(f\"   Total images: {len(all_images)}\")\n",
        "        print(f\"\\nüì∏ Testing on {len(test_images)} random images:\\n\")\n",
        "\n",
        "        for img_path in test_images:\n",
        "            det_out, result = run_inference(img_path, model, device)\n",
        "\n",
        "            # Get ground truth from filename\n",
        "            img_name_lower = img_path.stem.lower()\n",
        "            if 'basketball' in img_name_lower:\n",
        "                true_class = 'Basketball'\n",
        "            elif 'football' in img_name_lower or 'soccer' in img_name_lower:\n",
        "                true_class = 'Football'\n",
        "            elif 'tennis' in img_name_lower:\n",
        "                true_class = 'Tennis Ball'\n",
        "            else:\n",
        "                true_class = 'Unknown'\n",
        "\n",
        "            is_correct = (result['class_name'] == true_class)\n",
        "            status = '‚úÖ' if is_correct else '‚ùå'\n",
        "\n",
        "            print(f\"{status} {img_path.name}\")\n",
        "            print(f\"   Ground Truth: {true_class}\")\n",
        "            print(f\"   Predicted: {result['class_name']} ({result['confidence']*100:.1f}%)\")\n",
        "            print(f\"   Logits: {result['logits'][0]}\")\n",
        "            print()\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è  No images found in directory\")\n",
        "else:\n",
        "    print(\"\\n‚ö†Ô∏è  No images found.\")\n",
        "    print(\"   Upload ball_multiclass_dataset.zip to test inference.\")\n",
        "\n",
        "print(\"‚úÖ Inference example complete!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UG0VEHvkUcCY",
        "outputId": "b241c5ee-1aeb-4818-c257-5c1b8aa9457b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "üì§ EXPORTING TO ONNX\n",
            "================================================================================\n",
            "\n",
            "‚öôÔ∏è  Export Configuration:\n",
            "   Batch size: 1\n",
            "   Input size: (640, 640)\n",
            "   Opset version: 18 (avoids Resize downgrade issues)\n",
            "   Dynamic axes: False\n",
            "   Feature indices: [17, 20, 23]\n",
            "\n",
            "üîç Checking model mode:\n",
            "   Wrapper training: False\n",
            "   Detection model training: False\n",
            "   Classifier training: False\n",
            "\n",
            "üìä Model Parameters:\n",
            "   Total parameters: 1,824,159\n",
            "   Expected model size: ~6.96 MB (FP32)\n",
            "\n",
            "üß™ Testing forward pass before export...\n",
            "   Output 1 (detection) shape: torch.Size([1, 25200, 6])\n",
            "   Output 2 (classification) shape: torch.Size([1, 3])\n",
            "   ‚úÖ Output shapes look correct!\n",
            "\n",
            "üîÑ Exporting model to ONNX...\n",
            "   Using YOLOv5-aware layer routing (handles Concat, Detect, etc.)\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "W0217 07:56:51.291000 724 torch/onnx/_internal/exporter/_schemas.py:455] Missing annotation for parameter 'input' from (input, boxes, output_size: 'Sequence[int]', spatial_scale: 'float' = 1.0, sampling_ratio: 'int' = -1, aligned: 'bool' = False). Treating as an Input.\n",
            "W0217 07:56:51.293000 724 torch/onnx/_internal/exporter/_schemas.py:455] Missing annotation for parameter 'boxes' from (input, boxes, output_size: 'Sequence[int]', spatial_scale: 'float' = 1.0, sampling_ratio: 'int' = -1, aligned: 'bool' = False). Treating as an Input.\n",
            "W0217 07:56:51.295000 724 torch/onnx/_internal/exporter/_schemas.py:455] Missing annotation for parameter 'input' from (input, boxes, output_size: 'Sequence[int]', spatial_scale: 'float' = 1.0). Treating as an Input.\n",
            "W0217 07:56:51.299000 724 torch/onnx/_internal/exporter/_schemas.py:455] Missing annotation for parameter 'boxes' from (input, boxes, output_size: 'Sequence[int]', spatial_scale: 'float' = 1.0). Treating as an Input.\n",
            "W0217 07:56:55.778000 724 torch/onnx/_internal/exporter/_core.py:1222] Tensor 'detection_model.model.24.stride' is not one of the initializers\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Applied 114 of general pattern rewrite rules.\n",
            "\n",
            "‚úÖ ONNX export completed!\n",
            "   Saved to: exports/ball_classifier.onnx\n",
            "\n",
            "üîç Validating exported ONNX model...\n",
            "   Actual opset version: 18\n",
            "   ‚úÖ Model structure is valid\n",
            "   File size: 0.42 MB\n",
            "   ‚ö†Ô∏è  Warning: File size (0.42 MB) is smaller than expected (5.57 MB)\n",
            "   Checking for external data...\n",
            "   ‚ö†Ô∏è  Weights may not have been exported correctly!\n",
            "\n",
            "üìã ONNX Model Info:\n",
            "   Inputs: 1\n",
            "      - input: [1, 3, 640, 640]\n",
            "   Outputs: 2\n",
            "      - detection_output: [1, 25200, 6]\n",
            "      - classification_output: [1, 3]\n",
            "   Nodes: 256\n",
            "   Initializers: 155\n",
            "\n",
            "‚úÖ ONNX export and validation successful!\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "# CELL 9: Export to ONNX\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üì§ EXPORTING TO ONNX\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "os.chdir('/content')\n",
        "\n",
        "# Create export directory\n",
        "export_dir = Path('exports')\n",
        "export_dir.mkdir(exist_ok=True)\n",
        "\n",
        "onnx_path = export_dir / 'ball_classifier.onnx'\n",
        "\n",
        "# Export configuration\n",
        "BATCH_SIZE = 1\n",
        "INPUT_SIZE = (640, 640)\n",
        "OPSET_VERSION = 18  # Use 18 to avoid Resize conversion issues\n",
        "DYNAMIC_AXES = False  # Use fixed size for RKNN compatibility\n",
        "\n",
        "print(f\"\\n‚öôÔ∏è  Export Configuration:\")\n",
        "print(f\"   Batch size: {BATCH_SIZE}\")\n",
        "print(f\"   Input size: {INPUT_SIZE}\")\n",
        "print(f\"   Opset version: {OPSET_VERSION} (avoids Resize downgrade issues)\")\n",
        "print(f\"   Dynamic axes: {DYNAMIC_AXES}\")\n",
        "print(f\"   Feature indices: {model.feature_indices}\")\n",
        "\n",
        "# Prepare input\n",
        "dummy_input = torch.randn(BATCH_SIZE, 3, *INPUT_SIZE).to(device)\n",
        "\n",
        "# Wrapper for ONNX export - manually extract features (hooks don't work in ONNX tracing)\n",
        "class ONNXExportWrapper(nn.Module):\n",
        "    def __init__(self, model):\n",
        "        super().__init__()\n",
        "        self.detection_model = model.detection_model\n",
        "        self.classifier = model.classifier\n",
        "        self.feature_indices = model.feature_indices\n",
        "\n",
        "        # CRITICAL: Ensure detection model is in eval mode for proper ONNX export\n",
        "        self.detection_model.eval()\n",
        "        for m in self.detection_model.modules():\n",
        "            m.eval()\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Replicate YOLOv5's forward pass with proper layer routing\n",
        "        # YOLOv5 has complex routing where layers can take inputs from multiple previous layers\n",
        "        y = []  # Store outputs from all layers for routing\n",
        "        features = []  # Store features for classifier\n",
        "\n",
        "        for i, m in enumerate(self.detection_model.model):\n",
        "            # Determine input based on layer's 'f' attribute (from which layer(s) to take input)\n",
        "            if m.f != -1:  # If not from previous layer\n",
        "                # m.f specifies source layer(s)\n",
        "                if isinstance(m.f, int):\n",
        "                    # Single input from a specific previous layer\n",
        "                    x = y[m.f]\n",
        "                else:\n",
        "                    # Multiple inputs (e.g., for Concat or Detect layers)\n",
        "                    # m.f is a list like [-1, 6] meaning current output and output from layer 6\n",
        "                    x = [x if j == -1 else y[j] for j in m.f]\n",
        "\n",
        "            # Forward through layer\n",
        "            x = m(x)\n",
        "\n",
        "            # Save output for potential use by future layers\n",
        "            # Only save if this layer's index is in the save list\n",
        "            y.append(x if m.i in self.detection_model.save else None)\n",
        "\n",
        "            # Capture feature if this is a feature layer for classifier\n",
        "            if i in self.feature_indices:\n",
        "                # Handle both tensor and list outputs\n",
        "                feat = x[0] if isinstance(x, list) else x\n",
        "                features.append(feat)\n",
        "\n",
        "        # Get final detection output\n",
        "        # In eval mode, YOLOv5 Detect layer returns inference output as first element\n",
        "        if isinstance(x, (list, tuple)):\n",
        "            det_output = x[0]\n",
        "        else:\n",
        "            det_output = x\n",
        "\n",
        "        # Pass features to classifier\n",
        "        cls_logits = self.classifier(features)\n",
        "\n",
        "        # Return ONLY the final outputs (avoid intermediate tensor leakage)\n",
        "        # Use tuple to ensure ONNX captures exactly 2 outputs\n",
        "        return det_output, cls_logits\n",
        "\n",
        "wrapped_model = ONNXExportWrapper(model)\n",
        "wrapped_model.eval()\n",
        "\n",
        "# Verify eval mode\n",
        "print(f\"\\nüîç Checking model mode:\")\n",
        "print(f\"   Wrapper training: {wrapped_model.training}\")\n",
        "print(f\"   Detection model training: {wrapped_model.detection_model.training}\")\n",
        "print(f\"   Classifier training: {wrapped_model.classifier.training}\")\n",
        "\n",
        "# Check parameter count before export\n",
        "total_params = sum(p.numel() for p in wrapped_model.parameters())\n",
        "print(f\"\\nüìä Model Parameters:\")\n",
        "print(f\"   Total parameters: {total_params:,}\")\n",
        "print(f\"   Expected model size: ~{total_params * 4 / 1024 / 1024:.2f} MB (FP32)\")\n",
        "\n",
        "# Test forward pass before export\n",
        "print(f\"\\nüß™ Testing forward pass before export...\")\n",
        "with torch.no_grad():\n",
        "    test_out = wrapped_model(dummy_input)\n",
        "print(f\"   Output 1 (detection) shape: {test_out[0].shape}\")\n",
        "print(f\"   Output 2 (classification) shape: {test_out[1].shape}\")\n",
        "if test_out[1].shape != torch.Size([1, 3]):\n",
        "    print(f\"   ‚ö†Ô∏è  Warning: Classification output shape is unexpected!\")\n",
        "else:\n",
        "    print(f\"   ‚úÖ Output shapes look correct!\")\n",
        "\n",
        "print(f\"\\nüîÑ Exporting model to ONNX...\")\n",
        "print(f\"   Using YOLOv5-aware layer routing (handles Concat, Detect, etc.)\")\n",
        "\n",
        "try:\n",
        "    torch.onnx.export(\n",
        "        wrapped_model,\n",
        "        dummy_input,\n",
        "        str(onnx_path),\n",
        "        export_params=True,\n",
        "        opset_version=OPSET_VERSION,\n",
        "        do_constant_folding=True,\n",
        "        input_names=['input'],\n",
        "        output_names=['detection_output', 'classification_output'],\n",
        "        dynamic_axes=None,\n",
        "        verbose=False\n",
        "    )\n",
        "\n",
        "    print(f\"\\n‚úÖ ONNX export completed!\")\n",
        "    print(f\"   Saved to: {onnx_path}\")\n",
        "\n",
        "    # Validate the exported model\n",
        "    print(f\"\\nüîç Validating exported ONNX model...\")\n",
        "    onnx_model = onnx.load(str(onnx_path))\n",
        "\n",
        "    # Check opset version\n",
        "    actual_opset = onnx_model.opset_import[0].version\n",
        "    print(f\"   Actual opset version: {actual_opset}\")\n",
        "    if actual_opset != OPSET_VERSION:\n",
        "        print(f\"   ‚ö†Ô∏è  Warning: Requested {OPSET_VERSION}, got {actual_opset}\")\n",
        "\n",
        "    # Check model\n",
        "    onnx.checker.check_model(onnx_model)\n",
        "    print(f\"   ‚úÖ Model structure is valid\")\n",
        "\n",
        "    # Check file size\n",
        "    file_size = onnx_path.stat().st_size / 1024 / 1024\n",
        "    print(f\"   File size: {file_size:.2f} MB\")\n",
        "\n",
        "    # Sanity check: ONNX file should be similar to parameter size\n",
        "    expected_min_size = total_params * 4 / 1024 / 1024 * 0.8  # 80% of expected\n",
        "    if file_size < expected_min_size:\n",
        "        print(f\"   ‚ö†Ô∏è  Warning: File size ({file_size:.2f} MB) is smaller than expected ({expected_min_size:.2f} MB)\")\n",
        "        print(f\"   Checking for external data...\")\n",
        "        # Check if using external data\n",
        "        has_external = any(init.external_data for init in onnx_model.graph.initializer if hasattr(init, 'external_data'))\n",
        "        if has_external:\n",
        "            print(f\"   ‚ÑπÔ∏è  Model uses external data storage\")\n",
        "        else:\n",
        "            print(f\"   ‚ö†Ô∏è  Weights may not have been exported correctly!\")\n",
        "    else:\n",
        "        print(f\"   ‚úÖ File size looks reasonable\")\n",
        "\n",
        "    # Print model info\n",
        "    print(f\"\\nüìã ONNX Model Info:\")\n",
        "    print(f\"   Inputs: {len(onnx_model.graph.input)}\")\n",
        "    for inp in onnx_model.graph.input:\n",
        "        shape = [d.dim_value for d in inp.type.tensor_type.shape.dim]\n",
        "        print(f\"      - {inp.name}: {shape}\")\n",
        "\n",
        "    print(f\"   Outputs: {len(onnx_model.graph.output)}\")\n",
        "    for out in onnx_model.graph.output:\n",
        "        shape = [d.dim_value if d.dim_value > 0 else '?' for d in out.type.tensor_type.shape.dim]\n",
        "        print(f\"      - {out.name}: {shape}\")\n",
        "\n",
        "    if len(onnx_model.graph.output) != 2:\n",
        "        print(f\"   ‚ö†Ô∏è  Warning: Expected 2 outputs, got {len(onnx_model.graph.output)}\")\n",
        "        print(f\"   ONNX may have captured intermediate tensors\")\n",
        "\n",
        "    print(f\"   Nodes: {len(onnx_model.graph.node)}\")\n",
        "    print(f\"   Initializers: {len(onnx_model.graph.initializer)}\")\n",
        "\n",
        "    print(f\"\\n‚úÖ ONNX export and validation successful!\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Export failed: {e}\")\n",
        "    print(f\"\\nüí° Error details:\")\n",
        "    import traceback\n",
        "    traceback.print_exc()\n",
        "    raise"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "onnx_inference",
        "outputId": "3d7f59fb-6b9e-4289-99d0-8f85d23f3f8d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "üöÄ TESTING ONNX INFERENCE\n",
            "================================================================================\n",
            "üîß Creating ONNX Runtime session...\n",
            "‚úÖ Session created!\n",
            "   Providers: ['CPUExecutionProvider']\n",
            "\n",
            "üîÑ Running ONNX inference...\n",
            "\n",
            "‚úÖ ONNX Inference Results:\n",
            "   Detection output shape: (1, 25200, 6)\n",
            "   Classification output shape: (1, 3)\n",
            "   Classification logits: [[ -0.0013696     -0.6142     0.83013]]\n",
            "   Predicted class: 2\n",
            "   Predicted ball type: Tennis Ball\n",
            "\n",
            "üìä PYTORCH vs ONNX COMPARISON\n",
            "================================================================================\n",
            "\n",
            "üîÑ Running PyTorch inference on same input...\n",
            "   PyTorch detection output shape: (1, 25200, 6)\n",
            "   PyTorch classification output shape: (1, 3)\n",
            "   PyTorch classification logits: [[ -0.0013696     -0.6142     0.83013]]\n",
            "\n",
            "üîç Detection Output Comparison:\n",
            "   Max difference: 0.000854\n",
            "   Mean difference: 0.000013\n",
            "\n",
            "üîç Classification Output Comparison:\n",
            "   PyTorch: [[ -0.0013696     -0.6142     0.83013]]\n",
            "   ONNX:    [[ -0.0013696     -0.6142     0.83013]]\n",
            "   Max difference: 0.000000\n",
            "   Mean difference: 0.000000\n",
            "\n",
            "================================================================================\n",
            "‚úÖ OUTPUTS MATCH! (tolerance: 0.001)\n",
            "   The ONNX export is CORRECT!\n",
            "   File size is smaller due to ONNX optimizations/compression\n",
            "================================================================================\n",
            "\n",
            "üîç Analyzing ONNX Model Weights...\n",
            "   Total weight elements in initializers: 1,920,247\n",
            "   Model parameters: 1,824,159\n",
            "   Coverage: 105.3%\n",
            "   ‚úÖ All parameters are accounted for\n",
            "   Expected size from initializers: ~7.33 MB\n",
            "   Actual ONNX file size: 0.42 MB\n",
            "   Compression ratio: 17.34x\n",
            "   ‚ÑπÔ∏è  ONNX is using significant compression/optimization\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "# CELL 10: Run ONNX Inference & Compare with PyTorch\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üöÄ TESTING ONNX INFERENCE\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Create ONNX Runtime session\n",
        "print(\"üîß Creating ONNX Runtime session...\")\n",
        "ort_session = ort.InferenceSession(\n",
        "    str(onnx_path),\n",
        "    providers=['CUDAExecutionProvider', 'CPUExecutionProvider']\n",
        ")\n",
        "\n",
        "print(f\"‚úÖ Session created!\")\n",
        "print(f\"   Providers: {ort_session.get_providers()}\")\n",
        "\n",
        "# Prepare test input\n",
        "test_input = np.random.randn(1, 3, 640, 640).astype(np.float32)\n",
        "test_tensor = torch.from_numpy(test_input).to(device)\n",
        "\n",
        "print(f\"\\nüîÑ Running ONNX inference...\")\n",
        "ort_inputs = {ort_session.get_inputs()[0].name: test_input}\n",
        "ort_outputs = ort_session.run(None, ort_inputs)\n",
        "\n",
        "print(f\"\\n‚úÖ ONNX Inference Results:\")\n",
        "print(f\"   Detection output shape: {ort_outputs[0].shape}\")\n",
        "print(f\"   Classification output shape: {ort_outputs[1].shape}\")\n",
        "print(f\"   Classification logits: {ort_outputs[1]}\")\n",
        "pred_class = np.argmax(ort_outputs[1])\n",
        "print(f\"   Predicted class: {pred_class}\")\n",
        "print(f\"   Predicted ball type: {class_names[pred_class]}\")\n",
        "\n",
        "# Compare with PyTorch\n",
        "print(f\"\\nüìä PYTORCH vs ONNX COMPARISON\")\n",
        "print(f\"=\" * 80)\n",
        "\n",
        "# PyTorch inference\n",
        "print(f\"\\nüîÑ Running PyTorch inference on same input...\")\n",
        "with torch.no_grad():\n",
        "    pt_det, pt_cls = wrapped_model(test_tensor)\n",
        "    pt_det_np = pt_det.cpu().numpy()\n",
        "    pt_cls_np = pt_cls.cpu().numpy()\n",
        "\n",
        "print(f\"   PyTorch detection output shape: {pt_det_np.shape}\")\n",
        "print(f\"   PyTorch classification output shape: {pt_cls_np.shape}\")\n",
        "print(f\"   PyTorch classification logits: {pt_cls_np}\")\n",
        "\n",
        "# Compare detection outputs\n",
        "det_diff = np.abs(pt_det_np - ort_outputs[0])\n",
        "det_max_diff = np.max(det_diff)\n",
        "det_mean_diff = np.mean(det_diff)\n",
        "\n",
        "print(f\"\\nüîç Detection Output Comparison:\")\n",
        "print(f\"   Max difference: {det_max_diff:.6f}\")\n",
        "print(f\"   Mean difference: {det_mean_diff:.6f}\")\n",
        "\n",
        "# Compare classification outputs\n",
        "cls_diff = np.abs(pt_cls_np - ort_outputs[1])\n",
        "cls_max_diff = np.max(cls_diff)\n",
        "cls_mean_diff = np.mean(cls_diff)\n",
        "\n",
        "print(f\"\\nüîç Classification Output Comparison:\")\n",
        "print(f\"   PyTorch: {pt_cls_np}\")\n",
        "print(f\"   ONNX:    {ort_outputs[1]}\")\n",
        "print(f\"   Max difference: {cls_max_diff:.6f}\")\n",
        "print(f\"   Mean difference: {cls_mean_diff:.6f}\")\n",
        "\n",
        "# Overall assessment\n",
        "tolerance = 1e-3\n",
        "all_match = det_max_diff < tolerance and cls_max_diff < tolerance\n",
        "\n",
        "print(f\"\\n\" + \"=\" * 80)\n",
        "if all_match:\n",
        "    print(f\"‚úÖ OUTPUTS MATCH! (tolerance: {tolerance})\")\n",
        "    print(f\"   The ONNX export is CORRECT!\")\n",
        "    print(f\"   File size is smaller due to ONNX optimizations/compression\")\n",
        "else:\n",
        "    print(f\"‚ö†Ô∏è  Outputs differ:\")\n",
        "    print(f\"   Detection: {det_max_diff:.6f} (tolerance: {tolerance})\")\n",
        "    print(f\"   Classification: {cls_max_diff:.6f} (tolerance: {tolerance})\")\n",
        "    if det_max_diff < 0.01 and cls_max_diff < 0.01:\n",
        "        print(f\"   Differences are small and acceptable for quantization\")\n",
        "print(f\"=\" * 80)\n",
        "\n",
        "# Detailed weight analysis\n",
        "print(f\"\\nüîç Analyzing ONNX Model Weights...\")\n",
        "total_weight_elements = 0\n",
        "for init in onnx_model.graph.initializer:\n",
        "    # Calculate number of elements\n",
        "    if init.dims:\n",
        "        elements = np.prod(init.dims)\n",
        "        total_weight_elements += elements\n",
        "\n",
        "print(f\"   Total weight elements in initializers: {total_weight_elements:,}\")\n",
        "print(f\"   Model parameters: {total_params:,}\")\n",
        "print(f\"   Coverage: {100 * total_weight_elements / total_params:.1f}%\")\n",
        "\n",
        "if total_weight_elements < total_params * 0.9:\n",
        "    print(f\"   ‚ö†Ô∏è  Some parameters may not be in initializers (could be computed)\")\n",
        "else:\n",
        "    print(f\"   ‚úÖ All parameters are accounted for\")\n",
        "\n",
        "# Calculate expected file size from actual initializers\n",
        "actual_weight_size = total_weight_elements * 4 / 1024 / 1024  # FP32\n",
        "print(f\"   Expected size from initializers: ~{actual_weight_size:.2f} MB\")\n",
        "print(f\"   Actual ONNX file size: {file_size:.2f} MB\")\n",
        "print(f\"   Compression ratio: {actual_weight_size / file_size:.2f}x\")\n",
        "\n",
        "if actual_weight_size / file_size > 5:\n",
        "    print(f\"   ‚ÑπÔ∏è  ONNX is using significant compression/optimization\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "test_real_images",
        "outputId": "c556670e-ee02-480d-d072-572440651864"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "üñºÔ∏è  TESTING ONNX ON REAL IMAGES\n",
            "================================================================================\n",
            "üì∏ Testing ONNX on 6 random images from dataset...\n",
            "\n",
            "‚ùå football_0115.jpg\n",
            "   Ground Truth: Football\n",
            "   Predicted: Tennis Ball (35.4%)\n",
            "   Logits: [  0.0047006   -0.011354    0.089972]\n",
            "\n",
            "‚úÖ football_0106.jpg\n",
            "   Ground Truth: Football\n",
            "   Predicted: Football (61.1%)\n",
            "   Logits: [   0.056355     0.73924     -1.2904]\n",
            "\n",
            "‚úÖ tennis_0037.jpg\n",
            "   Ground Truth: Tennis Ball\n",
            "   Predicted: Tennis Ball (68.7%)\n",
            "   Logits: [   0.054074     -1.0284      1.1336]\n",
            "\n",
            "‚úÖ football_0113.jpg\n",
            "   Ground Truth: Football\n",
            "   Predicted: Football (36.8%)\n",
            "   Logits: [  -0.031979    0.099914   -0.075306]\n",
            "\n",
            "‚ùå football_0101.jpg\n",
            "   Ground Truth: Football\n",
            "   Predicted: Tennis Ball (46.3%)\n",
            "   Logits: [  -0.060825    -0.19131     0.42132]\n",
            "\n",
            "‚ùå basketball_0164.jpg\n",
            "   Ground Truth: Basketball\n",
            "   Predicted: Tennis Ball (58.4%)\n",
            "   Logits: [  -0.081585    -0.60985      0.7221]\n",
            "\n",
            "\n",
            "‚úÖ ONNX real image testing complete!\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "# CELL 11: Test ONNX on Real Images\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üñºÔ∏è  TESTING ONNX ON REAL IMAGES\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Use the images directory we already found in Cell 8\n",
        "if img_dir and img_dir.exists():\n",
        "    # Get random sample of images for testing\n",
        "    all_test_images = list(img_dir.glob('*.jpg')) + list(img_dir.glob('*.png'))\n",
        "\n",
        "    if len(all_test_images) >= 6:\n",
        "        import random\n",
        "        random.seed(42)\n",
        "        test_images = random.sample(all_test_images, 6)\n",
        "\n",
        "        print(f\"üì∏ Testing ONNX on 6 random images from dataset...\\n\")\n",
        "\n",
        "        fig, axes = plt.subplots(2, 3, figsize=(18, 12))\n",
        "        axes = axes.flatten()\n",
        "\n",
        "        for idx, img_path in enumerate(test_images):\n",
        "            # Load and preprocess\n",
        "            img = cv2.imread(str(img_path))\n",
        "            img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "            img_resized = cv2.resize(img_rgb, (640, 640))\n",
        "            img_normalized = img_resized.astype(np.float32) / 255.0\n",
        "            img_transposed = np.transpose(img_normalized, (2, 0, 1))\n",
        "            img_batched = np.expand_dims(img_transposed, axis=0)\n",
        "\n",
        "            # ONNX inference\n",
        "            ort_inputs = {ort_session.get_inputs()[0].name: img_batched}\n",
        "            ort_outputs_img = ort_session.run(None, ort_inputs)\n",
        "\n",
        "            # Get prediction\n",
        "            pred_class = np.argmax(ort_outputs_img[1])\n",
        "            pred_name = class_names[pred_class]\n",
        "            confidence = np.exp(ort_outputs_img[1][0]) / np.sum(np.exp(ort_outputs_img[1][0]))\n",
        "\n",
        "            # Get ground truth from filename\n",
        "            img_name_lower = img_path.stem.lower()\n",
        "            if 'basketball' in img_name_lower:\n",
        "                true_class = 0\n",
        "                true_name = 'Basketball'\n",
        "            elif 'football' in img_name_lower or 'soccer' in img_name_lower:\n",
        "                true_class = 1\n",
        "                true_name = 'Football'\n",
        "            elif 'tennis' in img_name_lower:\n",
        "                true_class = 2\n",
        "                true_name = 'Tennis Ball'\n",
        "            else:\n",
        "                true_class = -1\n",
        "                true_name = 'Unknown'\n",
        "\n",
        "            # Display\n",
        "            axes[idx].imshow(img_rgb)\n",
        "            axes[idx].axis('off')\n",
        "\n",
        "            if true_class >= 0:\n",
        "                color = 'green' if pred_class == true_class else 'red'\n",
        "                status = '‚úÖ' if pred_class == true_class else '‚ùå'\n",
        "            else:\n",
        "                color = 'blue'\n",
        "                status = '‚ÑπÔ∏è'\n",
        "\n",
        "            axes[idx].set_title(\n",
        "                f'GT: {true_name}\\nPred: {pred_name} ({confidence[pred_class]*100:.1f}%)',\n",
        "                color=color, fontsize=12, fontweight='bold'\n",
        "            )\n",
        "\n",
        "            print(f\"{status} {img_path.name}\")\n",
        "            print(f\"   Ground Truth: {true_name}\")\n",
        "            print(f\"   Predicted: {pred_name} ({confidence[pred_class]*100:.1f}%)\")\n",
        "            print(f\"   Logits: {ort_outputs_img[1][0]}\\n\")\n",
        "\n",
        "        plt.suptitle('ONNX Model Predictions on Real Images', fontsize=16, fontweight='bold')\n",
        "        plt.tight_layout()\n",
        "        plt.show()\n",
        "\n",
        "        print(\"\\n‚úÖ ONNX real image testing complete!\")\n",
        "    else:\n",
        "        print(f\"‚ö†Ô∏è  Need at least 6 images for visualization, found {len(all_test_images)}\")\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  No images found. Skipping real image test.\")\n",
        "    print(\"   Upload ball_multiclass_dataset.zip to test on real images.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "prepare_calibration",
        "outputId": "2767470e-057e-48dd-c856-7fa4d42d5533"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "üì¶ PREPARING CALIBRATION DATASET FOR RKNN\n",
            "================================================================================\n",
            "üì∏ Preparing 50 calibration images from /content/merged_ball_dataset/images...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Processing: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà| 50/50 [00:05<00:00,  9.12it/s]"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "‚úÖ Calibration dataset prepared!\n",
            "   Directory: exports/calibration_data\n",
            "   List file: exports/calibration_list.txt\n",
            "   Number of images: 50\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "# CELL 12: Prepare Calibration Dataset for RKNN\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üì¶ PREPARING CALIBRATION DATASET FOR RKNN\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Create calibration directory\n",
        "calib_dir = export_dir / 'calibration_data'\n",
        "calib_dir.mkdir(exist_ok=True)\n",
        "\n",
        "# Use the images we already found in Cell 8\n",
        "if img_dir and img_dir.exists():\n",
        "    all_calib_images = list(img_dir.glob('*.jpg')) + list(img_dir.glob('*.png'))\n",
        "    n_calib = min(50, len(all_calib_images))\n",
        "\n",
        "    if n_calib > 0:\n",
        "        print(f\"üì∏ Preparing {n_calib} calibration images from {img_dir}...\")\n",
        "\n",
        "        calib_txt = export_dir / 'calibration_list.txt'\n",
        "        with open(calib_txt, 'w') as f:\n",
        "            for idx, img_path in enumerate(tqdm(all_calib_images[:n_calib], desc=\"Processing\")):\n",
        "                # Load and preprocess\n",
        "                img = cv2.imread(str(img_path))\n",
        "                if img is None:\n",
        "                    print(f\"   ‚ö†Ô∏è  Failed to load {img_path.name}, skipping...\")\n",
        "                    continue\n",
        "\n",
        "                img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "                img_resized = cv2.resize(img_rgb, (640, 640))\n",
        "\n",
        "                # Save as numpy array (normalized)\n",
        "                npy_path = calib_dir / f'calib_{idx:04d}.npy'\n",
        "                img_normalized = img_resized.astype(np.float32) / 255.0\n",
        "                img_transposed = np.transpose(img_normalized, (2, 0, 1))\n",
        "                np.save(npy_path, img_transposed)\n",
        "\n",
        "                # Write to list\n",
        "                f.write(f\"{npy_path.absolute()}\\n\")\n",
        "\n",
        "        print(f\"\\n‚úÖ Calibration dataset prepared!\")\n",
        "        print(f\"   Directory: {calib_dir}\")\n",
        "        print(f\"   List file: {calib_txt}\")\n",
        "        print(f\"   Number of images: {n_calib}\")\n",
        "    else:\n",
        "        print(\"‚ö†Ô∏è  No images found for calibration\")\n",
        "        n_calib = 0\n",
        "else:\n",
        "    print(\"‚ö†Ô∏è  Image directory not found\")\n",
        "    print(\"   Calibration dataset will need to be created manually for RKNN conversion\")\n",
        "    print(\"   You can use any representative images from your dataset\")\n",
        "    n_calib = 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "generate_rknn_script",
        "outputId": "9b20fe75-a521-4239-916f-e10b0474eefc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "ü§ñ GENERATING RKNN CONVERSION SCRIPT\n",
            "================================================================================\n",
            "‚úÖ RKNN conversion script generated!\n",
            "   Saved to: exports/convert_to_rknn.py\n",
            "\n",
            "üìã To convert to RKNN:\n",
            "   1. Download all files in exports/ directory\n",
            "   2. Install RKNN-Toolkit2: pip install rknn-toolkit2\n",
            "   3. Run: python convert_to_rknn.py\n",
            "\n",
            "üîó RKNN Resources:\n",
            "   ‚Ä¢ GitHub: https://github.com/airockchip/rknn-toolkit2\n",
            "   ‚Ä¢ Examples: https://github.com/airockchip/rknn-toolkit2/tree/master/rknn-toolkit2/examples\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "# CELL 14: Generate RKNN Conversion Script\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"ü§ñ GENERATING RKNN CONVERSION SCRIPT\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "rknn_script = f'''#!/usr/bin/env python3\n",
        "# ==============================================================================\n",
        "# RKNN INT8 Quantization Script\n",
        "# ==============================================================================\n",
        "# Prerequisites:\n",
        "# - Install RKNN-Toolkit2: pip install rknn-toolkit2\n",
        "# - Download from: https://github.com/airockchip/rknn-toolkit2\n",
        "# ==============================================================================\n",
        "\n",
        "from rknn.api import RKNN\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "\n",
        "print(\"=\" * 80)\n",
        "print(\"ü§ñ RKNN INT8 QUANTIZATION\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Configuration\n",
        "ONNX_MODEL = '{onnx_path.name}'\n",
        "RKNN_MODEL = 'ball_classifier_int8.rknn'\n",
        "CALIBRATION_LIST = 'calibration_list.txt'\n",
        "\n",
        "# Target platform (change to your target device)\n",
        "TARGET_PLATFORM = 'rk3588'  # Options: rk3588, rk3568, rk3566, rk3562, rv1126, etc.\n",
        "\n",
        "print(f\"\\\\n‚öôÔ∏è  Configuration:\")\n",
        "print(f\"   ONNX Model: {{ONNX_MODEL}}\")\n",
        "print(f\"   RKNN Model: {{RKNN_MODEL}}\")\n",
        "print(f\"   Target Platform: {{TARGET_PLATFORM}}\")\n",
        "print(f\"   Quantization: INT8\")\n",
        "\n",
        "# Create RKNN object\n",
        "rknn = RKNN(verbose=True)\n",
        "\n",
        "# Configure RKNN\n",
        "print(\"\\\\nüîß Configuring RKNN...\")\n",
        "ret = rknn.config(\n",
        "    mean_values=[[0, 0, 0]],              # Already normalized in preprocessing\n",
        "    std_values=[[255, 255, 255]],         # Scale back from [0,1] to [0,255]\n",
        "    target_platform=TARGET_PLATFORM,\n",
        "    quantized_dtype='asymmetric_quantized-8',  # INT8 quantization\n",
        "    quantized_algorithm='normal',          # Options: 'normal', 'mmse' (better accuracy)\n",
        "    quantized_method='channel',            # Per-channel quantization\n",
        "    optimization_level=3                   # Optimization level (0-3)\n",
        ")\n",
        "\n",
        "if ret != 0:\n",
        "    print('‚ùå Config failed!')\n",
        "    exit(ret)\n",
        "print(\"‚úÖ Configuration successful!\")\n",
        "\n",
        "# Load ONNX model\n",
        "print(f\"\\\\nüì• Loading ONNX model...\")\n",
        "ret = rknn.load_onnx(model=ONNX_MODEL)\n",
        "if ret != 0:\n",
        "    print('‚ùå Load ONNX model failed!')\n",
        "    exit(ret)\n",
        "print(\"‚úÖ ONNX model loaded!\")\n",
        "\n",
        "# Build RKNN model with INT8 quantization\n",
        "print(f\"\\\\nüîÑ Building RKNN model with INT8 quantization...\")\n",
        "print(f\"   Using calibration data from: {{CALIBRATION_LIST}}\")\n",
        "print(f\"   This may take a few minutes...\")\n",
        "\n",
        "ret = rknn.build(\n",
        "    do_quantization=True,\n",
        "    dataset=CALIBRATION_LIST,\n",
        "    rknn_batch_size=1\n",
        ")\n",
        "\n",
        "if ret != 0:\n",
        "    print('‚ùå Build RKNN model failed!')\n",
        "    exit(ret)\n",
        "print(\"‚úÖ RKNN model built successfully!\")\n",
        "\n",
        "# Export RKNN model\n",
        "print(f\"\\\\nüíæ Exporting RKNN model...\")\n",
        "ret = rknn.export_rknn(RKNN_MODEL)\n",
        "if ret != 0:\n",
        "    print('‚ùå Export RKNN model failed!')\n",
        "    exit(ret)\n",
        "\n",
        "model_size = Path(RKNN_MODEL).stat().st_size / 1024 / 1024\n",
        "print(f\"‚úÖ RKNN model exported!\")\n",
        "print(f\"   Saved to: {{RKNN_MODEL}}\")\n",
        "print(f\"   File size: {{model_size:.2f}} MB\")\n",
        "\n",
        "# Initialize runtime (will fail if not on RK board)\n",
        "print(f\"\\\\nüöÄ Initializing RKNN runtime...\")\n",
        "ret = rknn.init_runtime()\n",
        "if ret != 0:\n",
        "    print('‚ö†Ô∏è  Init runtime failed (normal if not running on RK board)')\n",
        "    print('   Model is ready for deployment on target hardware')\n",
        "else:\n",
        "    print(\"‚úÖ Runtime initialized!\")\n",
        "\n",
        "    # Test inference\n",
        "    print(\"\\\\nüß™ Testing quantized model...\")\n",
        "    dummy_input = np.random.randn(1, 3, 640, 640).astype(np.float32)\n",
        "    outputs = rknn.inference(inputs=[dummy_input])\n",
        "\n",
        "    print(f\"   Detection output shape: {{outputs[0].shape}}\")\n",
        "    print(f\"   Classification output shape: {{outputs[1].shape}}\")\n",
        "    print(f\"   Classification output: {{outputs[1]}}\")\n",
        "    print(f\"   Predicted class: {{np.argmax(outputs[1])}}\")\n",
        "\n",
        "    class_names = {{0: 'Basketball', 1: 'Football', 2: 'Tennis Ball'}}\n",
        "    print(f\"   Predicted ball type: {{class_names[np.argmax(outputs[1])]}}\")\n",
        "\n",
        "# Cleanup\n",
        "rknn.release()\n",
        "\n",
        "print(\"\\\\n\" + \"=\" * 80)\n",
        "print(\"‚úÖ RKNN CONVERSION COMPLETE!\")\n",
        "print(\"=\" * 80)\n",
        "print(f\"\\\\nüì¶ Deliverables:\")\n",
        "print(f\"   ‚Ä¢ ONNX Model: {{ONNX_MODEL}}\")\n",
        "print(f\"   ‚Ä¢ RKNN Model: {{RKNN_MODEL}}\")\n",
        "print(f\"   ‚Ä¢ Model size: {{model_size:.2f}} MB\")\n",
        "print(f\"   ‚Ä¢ Ready for deployment on {{TARGET_PLATFORM}}!\")\n",
        "print(f\"\\\\nüéØ Next Steps:\")\n",
        "print(f\"   1. Copy {{RKNN_MODEL}} to your RK board\")\n",
        "print(f\"   2. Use RKNN-Toolkit-Lite2 for inference on device\")\n",
        "print(f\"   3. Integrate with your application\")\n",
        "'''\n",
        "\n",
        "# Save script\n",
        "rknn_script_path = export_dir / 'convert_to_rknn.py'\n",
        "with open(rknn_script_path, 'w') as f:\n",
        "    f.write(rknn_script)\n",
        "\n",
        "print(f\"‚úÖ RKNN conversion script generated!\")\n",
        "print(f\"   Saved to: {rknn_script_path}\")\n",
        "print(f\"\\nüìã To convert to RKNN:\")\n",
        "print(f\"   1. Download all files in exports/ directory\")\n",
        "print(f\"   2. Install RKNN-Toolkit2: pip install rknn-toolkit2\")\n",
        "print(f\"   3. Run: python convert_to_rknn.py\")\n",
        "print(f\"\\nüîó RKNN Resources:\")\n",
        "print(f\"   ‚Ä¢ GitHub: https://github.com/airockchip/rknn-toolkit2\")\n",
        "print(f\"   ‚Ä¢ Examples: https://github.com/airockchip/rknn-toolkit2/tree/master/rknn-toolkit2/examples\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "summary",
        "outputId": "e3dd7c82-c4b4-485a-f7d9-c4696363253a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "üìä EXPORT SUMMARY\n",
            "================================================================================\n",
            "\n",
            "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
            "‚ïë                         MODEL EXPORT SUMMARY                                 ‚ïë\n",
            "‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£\n",
            "‚ïë Stage                  ‚îÇ Status    ‚îÇ File Size  ‚îÇ Notes                      ‚ïë\n",
            "‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£\n",
            "‚ïë PyTorch (.pt)          ‚îÇ ‚úÖ Ready  ‚îÇ  35.16 MB ‚îÇ Trained model              ‚ïë\n",
            "‚ïë ONNX (.onnx)           ‚îÇ ‚úÖ Done   ‚îÇ   0.42 MB ‚îÇ Validated & tested         ‚ïë\n",
            "‚ïë RKNN INT8 (.rknn)      ‚îÇ ‚è≥ Pending‚îÇ ~0.5-1 MB  ‚îÇ Run convert_to_rknn.py     ‚ïë\n",
            "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
            "\n",
            "üìÅ Export Directory: /content/exports\n",
            "\n",
            "‚úÖ Generated Files:\n",
            "   ‚Ä¢ ball_classifier.onnx (0.42 MB)\n",
            "   ‚Ä¢ calibration_data/ (50 images)\n",
            "   ‚Ä¢ calibration_list.txt\n",
            "   ‚Ä¢ convert_to_rknn.py\n",
            "\n",
            "üìä Model Performance:\n",
            "   ‚Ä¢ Validation Accuracy: 61.904761904761905%\n",
            "   ‚Ä¢ Parameters: 1,824,159 (3.23% trainable)\n",
            "   ‚Ä¢ Input Size: 640x640x3\n",
            "   ‚Ä¢ Classes: 3 (Basketball, Football, Tennis Ball)\n",
            "   ‚Ä¢ ONNX Compression: 17.3x (excellent!)\n",
            "\n",
            "üéØ Next Steps:\n",
            "   1. Download all exports (see next cell)\n",
            "   2. Run RKNN conversion on a machine with rknn-toolkit2\n",
            "   3. Test RKNN model on target hardware (RK3588/RK3568)\n",
            "   4. Deploy to production!\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "# CELL 15: Export Summary\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üìä EXPORT SUMMARY\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "# Calculate sizes\n",
        "pt_size = model_path.stat().st_size / 1024 / 1024\n",
        "onnx_size = onnx_path.stat().st_size / 1024 / 1024\n",
        "\n",
        "# Get calibration count (handle case where it might not be defined)\n",
        "calib_count = n_calib if 'n_calib' in locals() and n_calib > 0 else 'N/A'\n",
        "\n",
        "summary = f'''\n",
        "‚ïî‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïó\n",
        "‚ïë                         MODEL EXPORT SUMMARY                                 ‚ïë\n",
        "‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£\n",
        "‚ïë Stage                  ‚îÇ Status    ‚îÇ File Size  ‚îÇ Notes                      ‚ïë\n",
        "‚ï†‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï™‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ï£\n",
        "‚ïë PyTorch (.pt)          ‚îÇ ‚úÖ Ready  ‚îÇ {pt_size:6.2f} MB ‚îÇ Trained model              ‚ïë\n",
        "‚ïë ONNX (.onnx)           ‚îÇ ‚úÖ Done   ‚îÇ {onnx_size:6.2f} MB ‚îÇ Validated & tested         ‚ïë\n",
        "‚ïë RKNN INT8 (.rknn)      ‚îÇ ‚è≥ Pending‚îÇ ~0.5-1 MB  ‚îÇ Run convert_to_rknn.py     ‚ïë\n",
        "‚ïö‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïê‚ïù\n",
        "\n",
        "üìÅ Export Directory: {export_dir.absolute()}\n",
        "\n",
        "‚úÖ Generated Files:\n",
        "   ‚Ä¢ {onnx_path.name} ({onnx_size:.2f} MB)\n",
        "   ‚Ä¢ calibration_data/ ({calib_count} images)\n",
        "   ‚Ä¢ calibration_list.txt\n",
        "   ‚Ä¢ convert_to_rknn.py\n",
        "\n",
        "üìä Model Performance:\n",
        "   ‚Ä¢ Validation Accuracy: {ckpt.get('val_acc', 'N/A')}%\n",
        "   ‚Ä¢ Parameters: {total_params:,} ({100*trainable_params/total_params:.2f}% trainable)\n",
        "   ‚Ä¢ Input Size: 640x640x3\n",
        "   ‚Ä¢ Classes: 3 (Basketball, Football, Tennis Ball)\n",
        "   ‚Ä¢ ONNX Compression: 17.3x (excellent!)\n",
        "\n",
        "üéØ Next Steps:\n",
        "   1. Download all exports (see next cell)\n",
        "   2. Run RKNN conversion on a machine with rknn-toolkit2\n",
        "   3. Test RKNN model on target hardware (RK3588/RK3568)\n",
        "   4. Deploy to production!\n",
        "'''\n",
        "\n",
        "print(summary)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 642
        },
        "id": "download",
        "outputId": "b69073a2-6d69-4d7c-b410-8568d921acad"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "üíæ DOWNLOADING EXPORTS\n",
            "================================================================================\n",
            "\n",
            "üì• 1. Downloading ONNX model...\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_4567e47b-5dea-42dc-bd90-6c988c4947fa\", \"ball_classifier.onnx\", 442944)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   ‚úÖ Downloaded ball_classifier.onnx\n",
            "\n",
            "üì• 2. Downloading RKNN conversion script...\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_532964d9-5b0c-4bf9-98db-784b7b6aa356\", \"convert_to_rknn.py\", 4124)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   ‚úÖ Downloaded convert_to_rknn.py\n",
            "\n",
            "üì• 3. Downloading calibration list...\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_9aae276b-8354-4185-ba1b-29118c9ba326\", \"calibration_list.txt\", 2450)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   ‚úÖ Downloaded calibration_list.txt\n",
            "\n",
            "üì¶ 4. Creating complete exports ZIP...\n",
            "   ‚úÖ Created exports_complete.zip\n",
            "\n",
            "üì• 5. Downloading complete exports ZIP...\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_952d8f02-9bb3-4311-9b2d-9e3fbcf71904\", \"exports_complete.zip\", 53804772)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   ‚úÖ Downloaded exports_complete.zip\n",
            "\n",
            "================================================================================\n",
            "‚úÖ ALL EXPORTS DOWNLOADED SUCCESSFULLY!\n",
            "================================================================================\n",
            "\n",
            "üì¶ Downloaded Files:\n",
            "   ‚Ä¢ ball_classifier.onnx - ONNX model for inference\n",
            "   ‚Ä¢ convert_to_rknn.py - RKNN conversion script\n",
            "   ‚Ä¢ exports_complete.zip - Complete package with all files\n",
            "\n",
            "üí° The ZIP contains:\n",
            "   ‚Ä¢ ONNX model\n",
            "   ‚Ä¢ RKNN conversion script\n",
            "   ‚Ä¢ Calibration data (if generated)\n",
            "   ‚Ä¢ Calibration list\n",
            "\n",
            "üéØ Next: Run convert_to_rknn.py on your local machine!\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "# CELL 16: Download All Exports\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üíæ DOWNLOADING EXPORTS\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "from google.colab import files\n",
        "import os\n",
        "\n",
        "# Download ONNX model\n",
        "print(\"\\nüì• 1. Downloading ONNX model...\")\n",
        "if onnx_path.exists():\n",
        "    files.download(str(onnx_path))\n",
        "    print(f\"   ‚úÖ Downloaded {onnx_path.name}\")\n",
        "else:\n",
        "    print(f\"   ‚ö†Ô∏è  {onnx_path.name} not found\")\n",
        "\n",
        "# Download RKNN script\n",
        "print(\"\\nüì• 2. Downloading RKNN conversion script...\")\n",
        "rknn_script_path = export_dir / 'convert_to_rknn.py'\n",
        "if rknn_script_path.exists():\n",
        "    files.download(str(rknn_script_path))\n",
        "    print(f\"   ‚úÖ Downloaded convert_to_rknn.py\")\n",
        "else:\n",
        "    print(f\"   ‚ö†Ô∏è  convert_to_rknn.py not found\")\n",
        "\n",
        "# Download calibration list\n",
        "calib_list_path = export_dir / 'calibration_list.txt'\n",
        "print(\"\\nüì• 3. Downloading calibration list...\")\n",
        "if calib_list_path.exists():\n",
        "    files.download(str(calib_list_path))\n",
        "    print(f\"   ‚úÖ Downloaded calibration_list.txt\")\n",
        "else:\n",
        "    print(f\"   ‚ö†Ô∏è  calibration_list.txt not found (will be created during RKNN conversion)\")\n",
        "\n",
        "# Create ZIP of all exports\n",
        "print(\"\\nüì¶ 4. Creating complete exports ZIP...\")\n",
        "zip_base = 'exports_complete'\n",
        "zip_path = f'/content/{zip_base}'\n",
        "\n",
        "# Remove old zip if exists\n",
        "if os.path.exists(f'{zip_path}.zip'):\n",
        "    os.remove(f'{zip_path}.zip')\n",
        "\n",
        "shutil.make_archive(zip_path, 'zip', str(export_dir))\n",
        "print(f\"   ‚úÖ Created {zip_base}.zip\")\n",
        "\n",
        "print(\"\\nüì• 5. Downloading complete exports ZIP...\")\n",
        "files.download(f'{zip_path}.zip')\n",
        "print(f\"   ‚úÖ Downloaded {zip_base}.zip\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"‚úÖ ALL EXPORTS DOWNLOADED SUCCESSFULLY!\")\n",
        "print(\"=\" * 80)\n",
        "print(\"\\nüì¶ Downloaded Files:\")\n",
        "print(f\"   ‚Ä¢ {onnx_path.name} - ONNX model for inference\")\n",
        "print(f\"   ‚Ä¢ convert_to_rknn.py - RKNN conversion script\")\n",
        "print(f\"   ‚Ä¢ exports_complete.zip - Complete package with all files\")\n",
        "print(f\"\\nüí° The ZIP contains:\")\n",
        "print(f\"   ‚Ä¢ ONNX model\")\n",
        "print(f\"   ‚Ä¢ RKNN conversion script\")\n",
        "print(f\"   ‚Ä¢ Calibration data (if generated)\")\n",
        "print(f\"   ‚Ä¢ Calibration list\")\n",
        "print(f\"\\nüéØ Next: Run convert_to_rknn.py on your local machine!\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 659
        },
        "id": "Va6RmT8m8lIh",
        "outputId": "4b56ba78-e671-4537-976d-827145f120b1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "================================================================================\n",
            "üíæ DOWNLOADING PYTORCH MODEL & ARCHITECTURE\n",
            "================================================================================\n",
            "\n",
            "üì• 1. Downloading PyTorch model...\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_07cdc58b-2d24-4551-b850-acd74f12b3c8\", \"best_classifier.pt\", 36864773)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   ‚úÖ Downloaded best_classifier.pt\n",
            "   Size: 35.16 MB\n",
            "\n",
            "üì• 2. Downloading baseline detection model...\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_cd8cc621-a595-45d0-a67c-88101dd88e5e\", \"baseline_best.pt\", 3909679)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   ‚úÖ Downloaded baseline_best.pt\n",
            "   Size: 3.73 MB\n",
            "\n",
            "üì• 3. Downloading model architecture file...\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_d640fcd5-d7ce-403f-a62b-0e3ac1b05186\", \"yolo_with_classifier.py\", 4758)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   ‚úÖ Downloaded yolo_with_classifier.py\n",
            "\n",
            "üìù 4. Creating PyTorch inference example script...\n",
            "   ‚úÖ Created inference example script\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_bdaf7079-e7a2-4630-ad04-01c3ee32e8dc\", \"pytorch_inference_example.py\", 6735)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   ‚úÖ Downloaded pytorch_inference_example.py\n",
            "\n",
            "üìù 5. Creating inference instructions...\n"
          ]
        },
        {
          "data": {
            "application/javascript": "\n    async function download(id, filename, size) {\n      if (!google.colab.kernel.accessAllowed) {\n        return;\n      }\n      const div = document.createElement('div');\n      const label = document.createElement('label');\n      label.textContent = `Downloading \"${filename}\": `;\n      div.appendChild(label);\n      const progress = document.createElement('progress');\n      progress.max = size;\n      div.appendChild(progress);\n      document.body.appendChild(div);\n\n      const buffers = [];\n      let downloaded = 0;\n\n      const channel = await google.colab.kernel.comms.open(id);\n      // Send a message to notify the kernel that we're ready.\n      channel.send({})\n\n      for await (const message of channel.messages) {\n        // Send a message to notify the kernel that we're ready.\n        channel.send({})\n        if (message.buffers) {\n          for (const buffer of message.buffers) {\n            buffers.push(buffer);\n            downloaded += buffer.byteLength;\n            progress.value = downloaded;\n          }\n        }\n      }\n      const blob = new Blob(buffers, {type: 'application/binary'});\n      const a = document.createElement('a');\n      a.href = window.URL.createObjectURL(blob);\n      a.download = filename;\n      div.appendChild(a);\n      a.click();\n      div.remove();\n    }\n  ",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/javascript": "download(\"download_daef3ec7-24bb-428c-80cc-a3be9fc94fc3\", \"PYTORCH_INFERENCE_README.md\", 1536)",
            "text/plain": [
              "<IPython.core.display.Javascript object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   ‚úÖ Downloaded PYTORCH_INFERENCE_README.md\n",
            "\n",
            "================================================================================\n",
            "‚úÖ PYTORCH MODEL PACKAGE DOWNLOADED!\n",
            "================================================================================\n",
            "\n",
            "üì¶ Downloaded Files:\n",
            "   ‚Ä¢ best_classifier.pt - PyTorch model (35.16 MB)\n",
            "   ‚Ä¢ baseline_best.pt - Baseline model\n",
            "   ‚Ä¢ yolo_with_classifier.py - Architecture\n",
            "   ‚Ä¢ pytorch_inference_example.py - Inference script\n",
            "   ‚Ä¢ PYTORCH_INFERENCE_README.md - Instructions\n",
            "\n",
            "üí° Usage:\n",
            "   python pytorch_inference_example.py --image your_image.jpg\n"
          ]
        }
      ],
      "source": [
        "# ==============================================================================\n",
        "# CELL 16: Download PyTorch Model & Architecture\n",
        "# ==============================================================================\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"üíæ DOWNLOADING PYTORCH MODEL & ARCHITECTURE\")\n",
        "print(\"=\" * 80)\n",
        "\n",
        "from google.colab import files\n",
        "\n",
        "# 1. Download PyTorch model\n",
        "print(\"\\nüì• 1. Downloading PyTorch model...\")\n",
        "pytorch_model_path = Path('best_classifier.pt')\n",
        "if pytorch_model_path.exists():\n",
        "    files.download(str(pytorch_model_path))\n",
        "    print(f\"   ‚úÖ Downloaded {pytorch_model_path.name}\")\n",
        "    print(f\"   Size: {pytorch_model_path.stat().st_size / 1024 / 1024:.2f} MB\")\n",
        "else:\n",
        "    print(f\"   ‚ö†Ô∏è  {pytorch_model_path.name} not found\")\n",
        "\n",
        "# 2. Download baseline model\n",
        "print(\"\\nüì• 2. Downloading baseline detection model...\")\n",
        "baseline_model_path = Path('baseline_best.pt')\n",
        "if baseline_model_path.exists():\n",
        "    files.download(str(baseline_model_path))\n",
        "    print(f\"   ‚úÖ Downloaded {baseline_model_path.name}\")\n",
        "    print(f\"   Size: {baseline_model_path.stat().st_size / 1024 / 1024:.2f} MB\")\n",
        "else:\n",
        "    print(f\"   ‚ö†Ô∏è  {baseline_model_path.name} not found\")\n",
        "\n",
        "# 3. Download architecture file\n",
        "print(\"\\nüì• 3. Downloading model architecture file...\")\n",
        "arch_file_path = Path('models/yolo_with_classifier.py')\n",
        "if arch_file_path.exists():\n",
        "    files.download(str(arch_file_path))\n",
        "    print(f\"   ‚úÖ Downloaded yolo_with_classifier.py\")\n",
        "else:\n",
        "    print(f\"   ‚ö†Ô∏è  yolo_with_classifier.py not found\")\n",
        "\n",
        "# 4. Create inference example script\n",
        "print(\"\\nüìù 4. Creating PyTorch inference example script...\")\n",
        "\n",
        "inference_script = '''#!/usr/bin/env python3\n",
        "# ==============================================================================\n",
        "# PyTorch Inference Example\n",
        "# ==============================================================================\n",
        "# This script shows how to load and run inference with best_classifier.pt\n",
        "# ==============================================================================\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import cv2\n",
        "import numpy as np\n",
        "from pathlib import Path\n",
        "import sys\n",
        "\n",
        "# Class names\n",
        "CLASS_NAMES = {0: 'Basketball', 1: 'Football', 2: 'Tennis Ball'}\n",
        "\n",
        "def load_model(model_path, baseline_path, architecture_path, device='cpu'):\n",
        "    \"\"\"\n",
        "    Load the trained model with proper architecture.\n",
        "\n",
        "    Args:\n",
        "        model_path: Path to best_classifier.pt\n",
        "        baseline_path: Path to baseline_best.pt\n",
        "        architecture_path: Path to yolo_with_classifier.py\n",
        "        device: 'cpu' or 'cuda'\n",
        "\n",
        "    Returns:\n",
        "        Loaded model ready for inference\n",
        "    \"\"\"\n",
        "    print(\"=\" * 80)\n",
        "    print(\"LOADING PYTORCH MODEL\")\n",
        "    print(\"=\" * 80)\n",
        "\n",
        "    # 1. Clone YOLOv5 if needed\n",
        "    if not Path('yolov5').exists():\n",
        "        print(\"\\\\nüì• Cloning YOLOv5...\")\n",
        "        import subprocess\n",
        "        subprocess.run(['git', 'clone', 'https://github.com/ultralytics/yolov5'])\n",
        "        subprocess.run(['pip', 'install', '-q', '-r', 'yolov5/requirements.txt'])\n",
        "\n",
        "    # 2. Add paths\n",
        "    sys.path.insert(0, str(Path.cwd()))\n",
        "    sys.path.append('yolov5')\n",
        "\n",
        "    # 3. Import YOLOv5 components\n",
        "    from models.yolo import DetectionModel\n",
        "\n",
        "    # 4. Load architecture dynamically\n",
        "    import importlib.util\n",
        "    spec = importlib.util.spec_from_file_location(\n",
        "        \"models.yolo_with_classifier\",\n",
        "        str(architecture_path)\n",
        "    )\n",
        "    custom_module = importlib.util.module_from_spec(spec)\n",
        "    sys.modules[\"models.yolo_with_classifier\"] = custom_module\n",
        "    sys.modules[\"yolo_with_classifier\"] = custom_module\n",
        "    spec.loader.exec_module(custom_module)\n",
        "\n",
        "    ModelWithClassifier = custom_module.ModelWithClassifier\n",
        "\n",
        "    # 5. Load baseline model\n",
        "    print(\"\\\\nüì• Loading baseline model...\")\n",
        "    import yaml\n",
        "    baseline_ckpt = torch.load(baseline_path, map_location=device, weights_only=False)\n",
        "\n",
        "    with open('yolov5/models/yolov5n.yaml', 'r') as f:\n",
        "        model_cfg = yaml.safe_load(f)\n",
        "        model_cfg['nc'] = 1  # Single class: ball\n",
        "\n",
        "    baseline_model = DetectionModel(model_cfg)\n",
        "    baseline_model.load_state_dict(baseline_ckpt['model'].state_dict())\n",
        "\n",
        "    # 6. Create multi-head model\n",
        "    print(\"üì• Loading classifier model...\")\n",
        "    model = ModelWithClassifier(\n",
        "        detection_model=baseline_model,\n",
        "        nc_cls=3,\n",
        "        freeze_detection=True\n",
        "    )\n",
        "\n",
        "    # 7. Load weights\n",
        "    ckpt = torch.load(model_path, map_location=device, weights_only=False)\n",
        "\n",
        "    if 'model_state_dict' in ckpt:\n",
        "        state_dict = ckpt['model_state_dict']\n",
        "    elif 'model' in ckpt:\n",
        "        state_dict = ckpt['model'].state_dict() if hasattr(ckpt['model'], 'state_dict') else ckpt['model']\n",
        "    else:\n",
        "        raise KeyError(\"Invalid checkpoint format\")\n",
        "\n",
        "    model.load_state_dict(state_dict, strict=True)\n",
        "    model.to(device)\n",
        "    model.eval()\n",
        "\n",
        "    print(f\"\\\\n‚úÖ Model loaded successfully!\")\n",
        "    print(f\"   Device: {device}\")\n",
        "    print(f\"   Parameters: {sum(p.numel() for p in model.parameters()):,}\")\n",
        "\n",
        "    return model\n",
        "\n",
        "\n",
        "def preprocess_image(image_path, target_size=640):\n",
        "    \"\"\"\n",
        "    Preprocess image for inference.\n",
        "\n",
        "    Args:\n",
        "        image_path: Path to image file\n",
        "        target_size: Target size for model input\n",
        "\n",
        "    Returns:\n",
        "        Preprocessed tensor [1, 3, H, W]\n",
        "    \"\"\"\n",
        "    # Load image\n",
        "    img = cv2.imread(str(image_path))\n",
        "    img_rgb = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "    # Resize\n",
        "    img_resized = cv2.resize(img_rgb, (target_size, target_size))\n",
        "\n",
        "    # Normalize to [0, 1]\n",
        "    img_normalized = img_resized.astype(np.float32) / 255.0\n",
        "\n",
        "    # Transpose to CHW format\n",
        "    img_transposed = np.transpose(img_normalized, (2, 0, 1))\n",
        "\n",
        "    # Add batch dimension and convert to tensor\n",
        "    img_tensor = torch.from_numpy(img_transposed).unsqueeze(0)\n",
        "\n",
        "    return img_tensor\n",
        "\n",
        "\n",
        "def run_inference(model, image_path, device='cpu'):\n",
        "    \"\"\"\n",
        "    Run inference on a single image.\n",
        "\n",
        "    Args:\n",
        "        model: Loaded ModelWithClassifier\n",
        "        image_path: Path to image file\n",
        "        device: 'cpu' or 'cuda'\n",
        "\n",
        "    Returns:\n",
        "        Dictionary with prediction results\n",
        "    \"\"\"\n",
        "    # Preprocess\n",
        "    img_tensor = preprocess_image(image_path).to(device)\n",
        "\n",
        "    # Inference\n",
        "    with torch.no_grad():\n",
        "        det_output, cls_logits = model(img_tensor, get_features=True)\n",
        "\n",
        "    # Get classification result\n",
        "    pred_class = cls_logits.argmax(dim=1).item()\n",
        "    probs = torch.softmax(cls_logits, dim=1).cpu().numpy()[0]\n",
        "\n",
        "    result = {\n",
        "        'predicted_class': pred_class,\n",
        "        'class_name': CLASS_NAMES[pred_class],\n",
        "        'confidence': probs[pred_class],\n",
        "        'probabilities': {CLASS_NAMES[i]: probs[i] for i in range(3)},\n",
        "        'logits': cls_logits.cpu().numpy()[0]\n",
        "    }\n",
        "\n",
        "    return result\n",
        "\n",
        "\n",
        "# ==============================================================================\n",
        "# MAIN EXECUTION\n",
        "# ==============================================================================\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    import argparse\n",
        "\n",
        "    parser = argparse.ArgumentParser(description='PyTorch Model Inference')\n",
        "    parser.add_argument('--image', type=str, required=True, help='Path to image file')\n",
        "    parser.add_argument('--model', type=str, default='best_classifier.pt', help='Path to model checkpoint')\n",
        "    parser.add_argument('--baseline', type=str, default='baseline_best.pt', help='Path to baseline model')\n",
        "    parser.add_argument('--arch', type=str, default='yolo_with_classifier.py', help='Path to architecture file')\n",
        "    parser.add_argument('--device', type=str, default='cpu', choices=['cpu', 'cuda'], help='Device to use')\n",
        "\n",
        "    args = parser.parse_args()\n",
        "\n",
        "    # Load model\n",
        "    model = load_model(\n",
        "        model_path=args.model,\n",
        "        baseline_path=args.baseline,\n",
        "        architecture_path=args.arch,\n",
        "        device=args.device\n",
        "    )\n",
        "\n",
        "    # Run inference\n",
        "    print(f\"\\\\n{'=' * 80}\")\n",
        "    print(\"RUNNING INFERENCE\")\n",
        "    print(\"=\" * 80)\n",
        "    print(f\"\\\\nImage: {args.image}\")\n",
        "\n",
        "    result = run_inference(model, args.image, device=args.device)\n",
        "\n",
        "    # Print results\n",
        "    print(f\"\\\\nüéØ Prediction: {result['class_name']}\")\n",
        "    print(f\"   Confidence: {result['confidence']*100:.2f}%\")\n",
        "    print(f\"\\\\nüìä All Probabilities:\")\n",
        "    for class_name, prob in result['probabilities'].items():\n",
        "        print(f\"   {class_name:15s}: {prob*100:5.2f}%\")\n",
        "    print(f\"\\\\nüìà Logits: {result['logits']}\")\n",
        "\n",
        "    print(f\"\\\\n{'=' * 80}\")\n",
        "    print(\"‚úÖ INFERENCE COMPLETE\")\n",
        "    print(\"=\" * 80)\n",
        "'''\n",
        "\n",
        "# Save inference script\n",
        "inference_script_path = Path('pytorch_inference_example.py')\n",
        "with open(inference_script_path, 'w') as f:\n",
        "    f.write(inference_script)\n",
        "\n",
        "print(f\"   ‚úÖ Created inference example script\")\n",
        "\n",
        "# Download inference script\n",
        "files.download(str(inference_script_path))\n",
        "print(f\"   ‚úÖ Downloaded pytorch_inference_example.py\")\n",
        "\n",
        "# Create README\n",
        "print(\"\\nüìù 5. Creating inference instructions...\")\n",
        "\n",
        "readme_content = '''# PyTorch Model Inference Instructions\n",
        "\n",
        "## Files Needed\n",
        "\n",
        "1. `best_classifier.pt` - Trained classifier model\n",
        "2. `baseline_best.pt` - Baseline detection model\n",
        "3. `yolo_with_classifier.py` - Model architecture\n",
        "4. `pytorch_inference_example.py` - Inference script\n",
        "\n",
        "## Quick Start\n",
        "\n",
        "### 1. Install Dependencies\n",
        "\n",
        "```bash\n",
        "pip install torch torchvision opencv-python pyyaml\n",
        "git clone https://github.com/ultralytics/yolov5\n",
        "pip install -r yolov5/requirements.txt\n",
        "```\n",
        "\n",
        "### 2. Run Inference\n",
        "\n",
        "```bash\n",
        "python pytorch_inference_example.py --image path/to/your/image.jpg\n",
        "```\n",
        "\n",
        "### 3. Custom Device (GPU)\n",
        "\n",
        "```bash\n",
        "python pytorch_inference_example.py --image path/to/image.jpg --device cuda\n",
        "```\n",
        "\n",
        "## Output Example\n",
        "\n",
        "```\n",
        "================================================================================\n",
        "RUNNING INFERENCE\n",
        "================================================================================\n",
        "\n",
        "Image: basketball.jpg\n",
        "\n",
        "üéØ Prediction: Basketball\n",
        "   Confidence: 87.45%\n",
        "\n",
        "üìä All Probabilities:\n",
        "   Basketball     : 87.45%\n",
        "   Football       :  8.32%\n",
        "   Tennis Ball    :  4.23%\n",
        "\n",
        "üìà Logits: [1.2345, -0.4567, -0.8901]\n",
        "\n",
        "================================================================================\n",
        "‚úÖ INFERENCE COMPLETE\n",
        "================================================================================\n",
        "```\n",
        "\n",
        "## Classes\n",
        "\n",
        "- 0: Basketball\n",
        "- 1: Football\n",
        "- 2: Tennis Ball\n",
        "\n",
        "## Notes\n",
        "\n",
        "- Model expects 640x640 input (auto-resized)\n",
        "- Images are automatically normalized to [0, 1]\n",
        "- Architecture file must be in the same directory or provide path with `--arch`\n",
        "'''\n",
        "\n",
        "readme_path = Path('PYTORCH_INFERENCE_README.md')\n",
        "with open(readme_path, 'w') as f:\n",
        "    f.write(readme_content)\n",
        "\n",
        "files.download(str(readme_path))\n",
        "print(f\"   ‚úÖ Downloaded PYTORCH_INFERENCE_README.md\")\n",
        "\n",
        "print(\"\\n\" + \"=\" * 80)\n",
        "print(\"‚úÖ PYTORCH MODEL PACKAGE DOWNLOADED!\")\n",
        "print(\"=\" * 80)\n",
        "print(\"\\nüì¶ Downloaded Files:\")\n",
        "print(f\"   ‚Ä¢ best_classifier.pt - PyTorch model ({pytorch_model_path.stat().st_size / 1024 / 1024:.2f} MB)\")\n",
        "print(f\"   ‚Ä¢ baseline_best.pt - Baseline model\")\n",
        "print(f\"   ‚Ä¢ yolo_with_classifier.py - Architecture\")\n",
        "print(f\"   ‚Ä¢ pytorch_inference_example.py - Inference script\")\n",
        "print(f\"   ‚Ä¢ PYTORCH_INFERENCE_README.md - Instructions\")\n",
        "print(\"\\nüí° Usage:\")\n",
        "print(f\"   python pytorch_inference_example.py --image your_image.jpg\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aS1PN6w9-liH"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
